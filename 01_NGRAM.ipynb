{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Words: From Strings to Tokens\n",
    "\n",
    "> *Before we dive into Transformers, let’s master the humble n-gram.*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is a “Word” in NLP?\n",
    "\n",
    "A “word” can be represented in at least three simple ways:  \n",
    "1. **String of characters** (e.g., `\"cat\"` → `['c','a','t']`)  \n",
    "2. **Index in a vocabulary** (e.g., `\"cat\"` → `537`)  \n",
    "3. **Dense vector embedding** (e.g., `\"cat\"` → `[0.12, –0.03, …]`)  \n",
    "\n",
    "Early models such as n-grams treat words purely as symbolic tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The n-Gram Model\n",
    "\n",
    "An n-gram model predicts the next word (token) based only on the previous *n–1* words:\n",
    "\n",
    "- **Unigram** (n=1): no history, just the overall frequency of each word  \n",
    "- **Bigram** (n=2): conditions on the one previous word  \n",
    "- **Trigram** (n=3): conditions on the two previous words  \n",
    "\n",
    "> **Note on smoothing:** In real corpora we use techniques like Laplace or Kneser–Ney smoothing to avoid zero probabilities, but we’ll ignore smoothing in this toy example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram Demo Code: What’s Happening and Why\n",
    "\n",
    "Below is a minimal Python script that demonstrates:\n",
    "\n",
    "1. **How we collect statistics** on word pairs (bigrams) from a tiny corpus.  \n",
    "2. **How we convert counts into probabilities**, forming a simple language model.  \n",
    "3. **How we sample** (“generate”) the next word given a history.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs\n",
      "like\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Minimal bigram language model demo\n",
    "from collections import Counter, defaultdict  # count occurrences, default zero for missing keys\n",
    "import random                               # random sampling utilities\n",
    "\n",
    "def train(bigrams):\n",
    "    \"\"\"\n",
    "    Given a list of (w1, w2) pairs, count how often each appears,\n",
    "    then compute P(w2 | w1) = count(w1, w2) / total_count(w1).\n",
    "    Returns a dict mapping (w1, w2) → probability.\n",
    "    \"\"\"\n",
    "    counts = Counter(bigrams)\n",
    "    totals = defaultdict(int)\n",
    "    for w1, w2 in bigrams:\n",
    "        totals[w1] += counts[(w1, w2)]\n",
    "    return {\n",
    "        (w1, w2): c / totals[w1]\n",
    "        for (w1, w2), c in counts.items()\n",
    "    }\n",
    "\n",
    "# Prepare a tiny corpus and extract all adjacent word pairs\n",
    "corpus = \"I like cats . I like dogs .\".split()\n",
    "bigrams = list(zip(corpus, corpus[1:]))\n",
    "\n",
    "# Train the model: learn P(next_word | current_word)\n",
    "prob = train(bigrams)\n",
    "\n",
    "def next_word(w1):\n",
    "    \"\"\"\n",
    "    Sample a next word given the previous word w1,\n",
    "    using the learned bigram probabilities.\n",
    "    \"\"\"\n",
    "    # Filter for bigrams that start with w1\n",
    "    choices = [(w2, p) for (a, w2), p in prob.items() if a == w1]\n",
    "    words, ps = zip(*choices)\n",
    "    return random.choices(words, ps)[0]\n",
    "\n",
    "# Try it out! Run and see which words are generated.\n",
    "print(next_word(\"like\"))   # → \"cats\" or \"dogs\"\n",
    "print(next_word(\"I\"))      # → \"like\"\n",
    "print(next_word(\"cats\"))   # → \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why n-grams alone aren’t enough**  \n",
    ">  \n",
    "> When words are treated as **isolated tokens**—whether as raw strings or as integer indexes—there is no built-in notion of similarity or meaning.  \n",
    ">  \n",
    "> - “dog” and “cat” are just two different symbols, no more related than “cat” and “salad.”  \n",
    "> - There’s no sense of context beyond the fixed window of n-grams.  \n",
    ">  \n",
    "> After all, we ultimately want a **model of word meaning** that can tell us:  \n",
    ">  \n",
    "> - Which words are similar in meaning (e.g. cat ≈ dog)  \n",
    "> - Which words are opposites (e.g. cold ↔ hot)  \n",
    "> - Which carry positive or negative connotations (e.g. happy vs. sad)  \n",
    "> - How different verbs relate to the same event from various perspectives (buy / sell / pay - If I buy something from you,\n",
    "you’ve probably sold it to me, and I likely paid you.)\n",
    ">  \n",
    "> A good semantic model should let us draw inferences for tasks like question answering, dialogue, or any meaning-driven application.  \n",
    ">  \n",
    "> In the next section, **02_Lexical_Semantics_Definitions**, we’ll explore foundational concepts from **lexical semantics**, the linguistic study of word meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-journey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
