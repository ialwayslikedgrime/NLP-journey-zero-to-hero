{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Embeddings: A Deep Dive into Sparse and Dense Representations\n",
    "\n",
    "Let me take you on a comprehensive journey through the world of embeddings, from their fundamental concepts to the sophisticated architectures that power modern AI systems. This is a fascinating topic that sits at the intersection of linguistics, mathematics, and neuroscience, and I'll help you understand not just what embeddings are, but why they work the way they do.\n",
    "\n",
    "## The Fundamental Problem: How Do We Represent Meaning?\n",
    "\n",
    "Before we dive into embeddings, let's consider the fundamental challenge we're trying to solve. Language is incredibly complex - words carry meaning, but that meaning is multifaceted, contextual, and often ambiguous. When we build AI systems that work with language, we need a way to represent this meaning in a format that computers can process: numbers.\n",
    "\n",
    "This is where embeddings come in. An embedding is essentially a numerical representation of meaning. But as we'll see, there are radically different approaches to creating these representations, each with its own strengths and limitations.\n",
    "\n",
    "## Sparse Embeddings: The Simplicity of One-Hot Encoding\n",
    "\n",
    "Let's start with sparse embeddings, which represent perhaps the most straightforward approach to encoding meaning. The key insight here is that sparse embeddings are **assigned**, not learned. This is a crucial distinction that sets them apart from their dense counterparts.\n",
    "\n",
    "### How Sparse Embeddings Work\n",
    "\n",
    "Imagine you're building a system that needs to understand text, and you have a vocabulary of 50,000 words. In sparse embedding, you create what's called a \"one-hot encoding\" for each word:\n",
    "\n",
    "1. First, you assign each word in your vocabulary a unique position\n",
    "2. Then, you represent each word as a vector with 50,000 dimensions\n",
    "3. For any given word, you put a \"1\" in its assigned position and \"0\" everywhere else\n",
    "\n",
    "For example:\n",
    "```\n",
    "Position 0: \"aardvark\" → [1, 0, 0, 0, ..., 0]\n",
    "Position 1: \"ability\"  → [0, 1, 0, 0, ..., 0]\n",
    "Position 2: \"able\"     → [0, 0, 1, 0, ..., 0]\n",
    "...\n",
    "Position 10,547: \"cat\" → [0, 0, 0, ..., 1, ..., 0]\n",
    "```\n",
    "\n",
    "The term \"sparse\" comes from the fact that these vectors are almost entirely zeros - they're \"sparse\" in their non-zero values.\n",
    "\n",
    "### Why Are Sparse Embeddings Assigned?\n",
    "\n",
    "The critical characteristic of sparse embeddings is that they're predetermined. When you decide that \"cat\" will be at position 10,547, that's a fixed assignment. There's no learning process that determines this - it's simply a lookup table. This has several important implications:\n",
    "\n",
    "1. **No semantic relationships**: The fact that \"cat\" is at position 10,547 and \"dog\" might be at position 15,832 tells us nothing about their semantic relationship\n",
    "2. **Orthogonality**: Each word's vector is orthogonal to every other word's vector (their dot product is zero), meaning the system treats all words as equally different from each other\n",
    "3. **Dimensionality curse**: You need as many dimensions as you have words in your vocabulary, which can become computationally expensive\n",
    "\n",
    "### Limitations of Sparse Embeddings\n",
    "\n",
    "While sparse embeddings are simple to understand and implement, they have significant limitations:\n",
    "\n",
    "1. **No notion of similarity**: \"Cat\" and \"kitten\" are as different as \"cat\" and \"democracy\" in this representation\n",
    "2. **Inefficient use of space**: Most values are zero, wasting computational resources\n",
    "3. **Cannot handle out-of-vocabulary words**: If a word isn't in your initial vocabulary, you can't represent it\n",
    "4. **No context sensitivity**: The same word always has the same representation, regardless of context\n",
    "\n",
    "## Dense Embeddings: Learning Meaning from Data\n",
    "\n",
    "This is where dense embeddings revolutionize how we represent language. Unlike sparse embeddings, dense embeddings are **learned** from data, not assigned. This fundamental difference leads to representations that capture semantic relationships and contextual nuances.\n",
    "\n",
    "### The Learning Process: From Random to Meaningful\n",
    "\n",
    "Let me walk you through how dense embeddings are actually learned:\n",
    "\n",
    "#### Step 1: Random Initialization\n",
    "\n",
    "When training begins, each word is assigned a random vector. These initial vectors are meaningless:\n",
    "\n",
    "```python\n",
    "# Initial random embeddings (simplified to 4 dimensions for illustration)\n",
    "embeddings = {\n",
    "    \"cat\":    [0.23, -0.45, 0.67, 0.12],\n",
    "    \"dog\":    [-0.34, 0.89, -0.21, 0.56],\n",
    "    \"kitten\": [0.78, -0.11, 0.33, -0.67],\n",
    "    \"car\":    [0.45, 0.23, -0.89, 0.34]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Step 2: Training Objective\n",
    "\n",
    "The model is given a task that requires understanding language. One common approach is the \"skip-gram\" objective: predict surrounding words given a center word. For instance, given the sentence \"The cat sits on the mat,\" the model might need to:\n",
    "- Given \"cat\" → predict \"The\", \"sits\", \"on\", \"the\", \"mat\"\n",
    "- Given \"sits\" → predict \"The\", \"cat\", \"on\", \"the\"\n",
    "\n",
    "#### Step 3: Learning Through Backpropagation\n",
    "\n",
    "Here's where the magic happens. When the model makes predictions:\n",
    "\n",
    "1. It uses the current embeddings to make predictions\n",
    "2. It calculates how wrong those predictions are (the loss)\n",
    "3. It adjusts the embeddings slightly to reduce this error\n",
    "4. This process repeats millions of times across massive text corpora\n",
    "\n",
    "Through this process, words that appear in similar contexts gradually develop similar embeddings. After training:\n",
    "\n",
    "```python\n",
    "# Learned embeddings (after millions of updates)\n",
    "embeddings = {\n",
    "    \"cat\":    [0.82, 0.31, -0.15, 0.44],  # Similar to other animals\n",
    "    \"dog\":    [0.79, 0.28, -0.18, 0.41],  # Similar to cat!\n",
    "    \"kitten\": [0.84, 0.33, -0.12, 0.46],  # Very similar to cat!\n",
    "    \"car\":    [0.12, -0.67, 0.89, -0.23]  # Different from animals\n",
    "}\n",
    "```\n",
    "\n",
    "### Why This Works: The Distributional Hypothesis\n",
    "\n",
    "The theoretical foundation for why this works is the \"distributional hypothesis\" - the idea that words with similar meanings appear in similar contexts. As the model sees millions of examples, it learns:\n",
    "\n",
    "- \"Cat\" and \"dog\" both appear near words like \"pet,\" \"feed,\" \"walk\"\n",
    "- \"Cat\" and \"car\" rarely appear in the same contexts\n",
    "- Therefore, \"cat\" and \"dog\" should have similar representations\n",
    "\n",
    "## Static vs. Contextual Embeddings: The Evolution of Understanding\n",
    "\n",
    "Now let's explore one of the most significant advances in embedding technology: the move from static to contextual embeddings.\n",
    "\n",
    "### Static Embeddings: One Word, One Vector\n",
    "\n",
    "Early dense embedding models like Word2Vec and GloVe created static embeddings. In these models:\n",
    "\n",
    "1. Each word has exactly one embedding\n",
    "2. This embedding is the same regardless of context\n",
    "3. The embedding is learned from all occurrences of the word in the training data\n",
    "\n",
    "This creates a fundamental problem with polysemy (words with multiple meanings):\n",
    "\n",
    "```python\n",
    "# Static embedding for \"bank\" - same in all contexts\n",
    "\"I deposited money at the bank\"     → \"bank\" = [0.43, -0.21, 0.67, ...]\n",
    "\"I sat by the river bank\"           → \"bank\" = [0.43, -0.21, 0.67, ...]\n",
    "\"The plane had to bank left\"        → \"bank\" = [0.43, -0.21, 0.67, ...]\n",
    "```\n",
    "\n",
    "The static embedding for \"bank\" is an average of all its meanings, which isn't ideal for any specific usage.\n",
    "\n",
    "### Contextual Embeddings: Dynamic Meaning Representation\n",
    "\n",
    "Modern architectures like BERT, GPT, and other transformer-based models introduced contextual embeddings. These models:\n",
    "\n",
    "1. Process entire sequences of text at once\n",
    "2. Generate different embeddings for the same word based on its context\n",
    "3. Capture nuanced meaning and grammatical roles\n",
    "\n",
    "```python\n",
    "# Contextual embeddings for \"bank\" - different in each context\n",
    "\"I deposited money at the bank\"     → \"bank\" = [0.43, -0.21, 0.67, ...]  # Financial institution\n",
    "\"I sat by the river bank\"           → \"bank\" = [0.78, 0.33, -0.15, ...]  # River edge\n",
    "\"The plane had to bank left\"        → \"bank\" = [-0.22, 0.56, 0.41, ...]  # Aviation maneuver\n",
    "```\n",
    "\n",
    "## Architectural Deep Dive: How Contextual Embeddings Work\n",
    "\n",
    "Let's examine how modern transformer architectures create these context-sensitive embeddings:\n",
    "\n",
    "### The Transformer Architecture\n",
    "\n",
    "The transformer architecture, introduced in the \"Attention is All You Need\" paper, revolutionized how we create embeddings. Here's how it works:\n",
    "\n",
    "#### 1. Input Processing\n",
    "```python\n",
    "# Input: \"The cat sits on the mat\"\n",
    "# Step 1: Tokenization\n",
    "tokens = [\"The\", \"cat\", \"sits\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Step 2: Initial embedding lookup (static embeddings)\n",
    "initial_embeddings = [embed_table[token] for token in tokens]\n",
    "\n",
    "# Step 3: Add positional information\n",
    "embeddings_with_position = [\n",
    "    initial_embed + positional_encoding(position)\n",
    "    for position, initial_embed in enumerate(initial_embeddings)\n",
    "]\n",
    "```\n",
    "\n",
    "#### 2. Self-Attention Mechanism\n",
    "\n",
    "The key innovation is self-attention, which allows each word to \"attend\" to every other word in the sequence:\n",
    "\n",
    "```python\n",
    "def self_attention(query, key, value):\n",
    "    # Compute attention scores\n",
    "    scores = matmul(query, key.transpose()) / sqrt(d_k)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    output = matmul(attention_weights, value)\n",
    "    \n",
    "    return output\n",
    "```\n",
    "\n",
    "This mechanism allows \"cat\" to gather information from all other words in the sentence, creating a context-aware representation.\n",
    "\n",
    "#### 3. Multiple Layers of Processing\n",
    "\n",
    "Transformers typically stack multiple layers of self-attention and feed-forward networks:\n",
    "\n",
    "```python\n",
    "def transformer_layer(embeddings):\n",
    "    # Multi-head self-attention\n",
    "    attended = multi_head_attention(embeddings)\n",
    "    \n",
    "    # Add & normalize\n",
    "    attended = layer_norm(embeddings + attended)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    output = feed_forward(attended)\n",
    "    \n",
    "    # Add & normalize again\n",
    "    output = layer_norm(attended + output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Stack multiple layers\n",
    "current_embeddings = initial_embeddings\n",
    "for _ in range(num_layers):  # Typically 12-24 layers\n",
    "    current_embeddings = transformer_layer(current_embeddings)\n",
    "```\n",
    "\n",
    "### Bidirectional vs. Unidirectional Models\n",
    "\n",
    "Different architectures process context differently:\n",
    "\n",
    "#### BERT (Bidirectional)\n",
    "- Looks at context from both directions simultaneously\n",
    "- Can see all words in the sequence when creating embeddings\n",
    "- Excellent for tasks requiring full sentence understanding\n",
    "\n",
    "```python\n",
    "# BERT can see the entire context\n",
    "\"The [MASK] sits on the mat\"  # Can use both \"The\" and \"sits on the mat\" to predict \"cat\"\n",
    "```\n",
    "\n",
    "#### GPT (Unidirectional)\n",
    "- Only looks at previous context (left-to-right)\n",
    "- Cannot see future words when creating embeddings\n",
    "- Designed for text generation tasks\n",
    "\n",
    "```python\n",
    "# GPT only sees previous context\n",
    "\"The cat sits\"  # When processing \"sits\", can only see \"The cat\", not what comes after\n",
    "```\n",
    "\n",
    "## The Mathematics Behind Embedding Learning\n",
    "\n",
    "To truly understand how embeddings are learned, let's look at the mathematical principles:\n",
    "\n",
    "### Loss Functions and Optimization\n",
    "\n",
    "The learning process is driven by loss functions that measure how well the model's predictions match reality:\n",
    "\n",
    "```python\n",
    "def skip_gram_loss(center_word_embedding, context_word_embedding, negative_samples):\n",
    "    # Positive example (words that actually appear together)\n",
    "    positive_score = sigmoid(dot_product(center_word_embedding, context_word_embedding))\n",
    "    \n",
    "    # Negative examples (random words that don't appear together)\n",
    "    negative_scores = [\n",
    "        sigmoid(-dot_product(center_word_embedding, neg_embedding))\n",
    "        for neg_embedding in negative_samples\n",
    "    ]\n",
    "    \n",
    "    # Loss encourages positive pairs to have high scores, negative pairs to have low scores\n",
    "    loss = -log(positive_score) - sum(log(score) for score in negative_scores)\n",
    "    \n",
    "    return loss\n",
    "```\n",
    "\n",
    "### Gradient Descent and Backpropagation\n",
    "\n",
    "The embeddings are updated through gradient descent:\n",
    "\n",
    "```python\n",
    "def update_embeddings(embeddings, gradients, learning_rate):\n",
    "    for word, gradient in gradients.items():\n",
    "        embeddings[word] -= learning_rate * gradient\n",
    "    return embeddings\n",
    "```\n",
    "\n",
    "This process gradually adjusts embeddings to minimize the loss function, leading to meaningful representations.\n",
    "\n",
    "## Advanced Concepts in Modern Embeddings\n",
    "\n",
    "### Subword Tokenization\n",
    "\n",
    "Modern models often use subword tokenization to handle:\n",
    "- Out-of-vocabulary words\n",
    "- Morphological relationships\n",
    "- Rare words\n",
    "\n",
    "```python\n",
    "# Example of subword tokenization\n",
    "\"unhappiness\" → [\"un\", \"happiness\"]\n",
    "\"preprocessing\" → [\"pre\", \"process\", \"ing\"]\n",
    "```\n",
    "\n",
    "This allows the model to understand new words by combining known subwords.\n",
    "\n",
    "### Cross-lingual Embeddings\n",
    "\n",
    "Some models create embeddings that work across languages:\n",
    "\n",
    "```python\n",
    "# Same concept, different languages, similar embeddings\n",
    "\"cat\" (English)   → [0.82, 0.31, -0.15, ...]\n",
    "\"chat\" (French)   → [0.79, 0.33, -0.17, ...]\n",
    "\"gato\" (Spanish)  → [0.81, 0.29, -0.16, ...]\n",
    "```\n",
    "\n",
    "This enables:\n",
    "- Machine translation\n",
    "- Cross-lingual information retrieval\n",
    "- Multilingual models\n",
    "\n",
    "## Practical Implications and Applications\n",
    "\n",
    "Understanding these embedding concepts is crucial for:\n",
    "\n",
    "1. **Search Engines**: Modern search engines use embeddings to understand query intent and document relevance\n",
    "2. **Recommendation Systems**: Netflix, Spotify, and others use embeddings to represent users and items\n",
    "3. **Chatbots and Virtual Assistants**: Contextual embeddings help understand user queries and generate appropriate responses\n",
    "4. **Machine Translation**: Cross-lingual embeddings facilitate translation between languages\n",
    "5. **Sentiment Analysis**: Embeddings capture emotional connotations of words and phrases\n",
    "\n",
    "## The Future of Embeddings\n",
    "\n",
    "The field continues to evolve rapidly:\n",
    "\n",
    "1. **Multimodal Embeddings**: Combining text, image, and audio representations\n",
    "2. **More Efficient Architectures**: Reducing computational costs while maintaining quality\n",
    "3. **Better Contextual Understanding**: Capturing even more nuanced meanings and relationships\n",
    "4. **Explainable Embeddings**: Making embedding spaces more interpretable\n",
    "\n",
    "## Conclusion: The Power of Learned Representations\n",
    "\n",
    "The journey from sparse, assigned embeddings to dense, learned, contextual embeddings represents one of the most significant advances in natural language processing. By allowing models to learn representations from data rather than relying on predetermined encodings, we've created systems that can:\n",
    "\n",
    "- Understand semantic relationships\n",
    "- Handle polysemy and context-dependent meanings\n",
    "- Process language with near-human levels of comprehension\n",
    "- Transfer knowledge across languages and domains\n",
    "\n",
    "The key insight is that meaning emerges from patterns of usage, and by training models to recognize these patterns, we create embeddings that capture the rich, multifaceted nature of human language. Whether you're building a search engine, a chatbot, or any other language-aware application, understanding these embedding concepts is fundamental to creating effective AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-journey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
