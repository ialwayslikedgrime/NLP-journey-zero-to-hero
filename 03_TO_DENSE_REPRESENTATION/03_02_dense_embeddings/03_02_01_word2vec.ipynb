{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: Understanding Dense Word Embeddings from the Ground Up\n",
    "\n",
    "## Introduction: Why Dense Embeddings Matter\n",
    "\n",
    "Imagine you're trying to teach a computer to understand language. You could represent each word as a unique ID number (like word #1234 = \"cat\"), but this tells us nothing about what words mean or how they relate to each other. This is the fundamental limitation of sparse representations that we've previously discussed.\n",
    "\n",
    "Dense embeddings revolutionize this by representing each word as a point in a continuous vector space - think of it as placing words in a multi-dimensional universe where similar words are close together and dissimilar words are far apart. Today, we'll dive deep into Word2Vec, one of the most influential algorithms for creating these dense embeddings.\n",
    "\n",
    "### 🧠 Learning Objective\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the mathematical foundations of Word2Vec\n",
    "2. Master both the Skip-gram and CBOW architectures \n",
    "3. Implement key components of Word2Vec from scratch\n",
    "4. Appreciate why Word2Vec was revolutionary for NLP\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: The Intuition Behind Word2Vec\n",
    "\n",
    "### From Sparse to Dense: A Paradigm Shift\n",
    "\n",
    "Let's start with a crucial insight: **words that appear in similar contexts tend to have similar meanings**. This is known as the distributional hypothesis, famously summarized by linguist J.R. Firth: \"You shall know a word by the company it keeps.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Innovation: Predicting Context from Words\n",
    "\n",
    "The Key Innovation: From Counting to Predicting\n",
    "This is a crucial concept, as it represents one of the most important paradigm shifts in NLP.\n",
    "\n",
    "### Understanding the Revolutionary Shift\n",
    "The Old Way: Counting Co-occurrences\n",
    "Before Word2Vec, most word representation methods relied on counting. Let me illustrate this with a concrete example:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix (partial):\n",
      "[[0 2 0 0 0]\n",
      " [2 0 1 0 1]\n",
      " [0 1 0 2 0]\n",
      " [0 0 2 0 2]\n",
      " [0 1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Traditional co-occurrence counting approach\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love deep learning\", \n",
    "    \"Machine learning is powerful\",\n",
    "    \"Deep learning requires data\"\n",
    "]\n",
    "\n",
    "# Make vocab all lowercase to match your .lower()’ed words:\n",
    "vocab = [\"i\", \"love\", \"machine\", \"learning\", \"deep\", \"is\", \"powerful\", \"requires\", \"data\"]\n",
    "\n",
    "cooccurrence_matrix = np.zeros((len(vocab), len(vocab)), dtype=int)\n",
    "\n",
    "window_size = 1\n",
    "for sentence in corpus:\n",
    "    words = sentence.lower().split()\n",
    "    for i, word1 in enumerate(words):\n",
    "        for j in range(max(0, i-window_size), min(len(words), i+window_size+1)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            idx1 = vocab.index(word1)\n",
    "            idx2 = vocab.index(words[j])\n",
    "            cooccurrence_matrix[idx1, idx2] += 1\n",
    "\n",
    "print(\"Co-occurrence matrix (partial):\")\n",
    "print(cooccurrence_matrix[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of the Traditional Approach\n",
    "\n",
    "This approach has several limitations:\n",
    "\n",
    "- **Sparsity**: Most word pairs never co-occur, leading to mostly zero-filled matrices  \n",
    "- **High dimensionality**: Each word needs a dimension for every other word  \n",
    "- **Poor generalization**: Can't infer relationships between words that don't directly co-occur  \n",
    "- **Memory intensive**: Storing these large, sparse matrices is inefficient  \n",
    "\n",
    "---\n",
    "\n",
    "## The New Way: Prediction as Learning\n",
    "\n",
    "Word2Vec's genius was to reformulate the problem.  \n",
    "Instead of asking **\"How often do words appear together?\"**, it asks:  \n",
    "**\"Can I predict which words will appear together?\"**\n",
    "\n",
    "Let me break this down with a deeper analogy:\n",
    "\n",
    "### 🧠 Enhanced Analogy\n",
    "\n",
    "Imagine you're learning a new language by living in a foreign country. You have two approaches:\n",
    "\n",
    "- **Approach 1 (Traditional)**:  \n",
    "  You carry a notebook and tally every time you hear word pairs together.  \n",
    "  \"Coffee\" appears with \"hot\" 23 times, with \"cup\" 18 times, etc.  \n",
    "  This gives you statistics but not understanding.\n",
    "\n",
    "- **Approach 2 (Word2Vec)**:  \n",
    "  You try to predict what words you'll hear next.  \n",
    "  When someone says *\"I'd like a cup of...\"*, you predict \"coffee\" or \"tea\".  \n",
    "  When you're wrong, you adjust your mental model.  \n",
    "  Over time, you develop an intuition that **\"coffee\"**, **\"tea\"**, and **\"water\"** are similar because they appear in similar contexts.\n",
    "\n",
    "---\n",
    "\n",
    "This **prediction-based approach** naturally captures **semantic relationships**,  \n",
    "because **words with similar meanings tend to appear in similar contexts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec's breakthrough was treating word embedding as a **prediction problem**. Instead of counting co-occurrences (like in sparse methods), Word2Vec:\n",
    "\n",
    "1. Takes a word as input\n",
    "2. Tries to predict its surrounding context words\n",
    "3. Adjusts word vectors to improve these predictions\n",
    "4. The resulting vectors capture semantic relationships!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📖 Understanding Word2Vec: The Complete Picture\n",
    "\n",
    "### 🎯 Now that we have an intuition for how Word2Vec differs from traditional sparse representations, let's give it a **clear, formal definition**.\n",
    "\n",
    "---\n",
    "\n",
    "### ❓ What Exactly is Word2Vec?\n",
    "\n",
    "**Word2Vec** is not a single algorithm, but rather a **family of model architectures** designed to learn **dense vector representations of words**, known as **word embeddings**.\n",
    "\n",
    "Think of Word2Vec as a **framework** for capturing semantic and syntactic relationships between words by leveraging the **distributional hypothesis**:  \n",
    "> *Words that occur in similar contexts tend to have similar meanings.*\n",
    "\n",
    "Introduced by **Tomas Mikolov et al.** at Google in **2013**, Word2Vec transforms each word in a corpus into a **low-dimensional, dense vector** that captures its contextual usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Word2Vec: The Complete Picture\n",
    "\n",
    "#### What Exactly is Word2Vec?\n",
    "\n",
    "Word2Vec is **not** a single algorithm—rather, it is a *family of model architectures* for learning dense, numerical representations of words (known as *word embeddings*). Think of Word2Vec as a toolkit that offers **two complementary ways** to learn these vectors:\n",
    "\n",
    "| Model Architecture | Core Idea | Direction of Prediction |\n",
    "|--------------------|-----------|-------------------------|\n",
    "| **Skip‑gram** | Uses one *target* word to predict the words that appear around it. | **Target → Context** |\n",
    "| **Continuous Bag of Words (CBOW)** | Uses the surrounding *context* words to predict the missing target word. | **Context → Target** |\n",
    "\n",
    "When people say **“Word2Vec,”** they’re talking about this overall framework, first introduced by *Mikolov et al.* at Google in 2013. The name literally means **“Word to Vector”**—turning words into vectors that a machine‑learning model can manipulate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do we mean by “model architecture”?\n",
    "\n",
    "In machine‑learning jargon, a **model architecture** is the **high‑level blueprint** that specifies:\n",
    "\n",
    "- **Input & output format** – what goes in, what comes out (e.g., a word vs. its context).\n",
    "- **Computation flow** – the sequence of layers or operations that transform inputs into outputs.\n",
    "- **Objective function** – the metric the model optimizes (e.g., predicting surrounding words).\n",
    "\n",
    "In **Word2Vec**, **Skip‑gram** and **CBOW** are two different architectures because they have opposite prediction goals and therefore different data flows, even though they share the same overall purpose (learning word embeddings). This distinction matters because it affects training speed, data efficiency, and the kinds of semantic relationships that emerge in the learned vectors.\n",
    "\n",
    "Let's illustrate this relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Framework initialized with skip-gram architecture\n",
      "\n",
      "        Word2Vec Framework\n",
      "        ├── Skip-gram Model\n",
      "        │   └── Predicts context from target\n",
      "        │       Example: \"fox\" → [\"quick\", \"brown\", \"jumps\", \"over\"]\n",
      "        │\n",
      "        └── CBOW Model\n",
      "            └── Predicts target from context\n",
      "                Example: [\"quick\", \"brown\", \"jumps\", \"over\"] → \"fox\"\n",
      "        \n",
      "        Both models share:\n",
      "        - The goal of learning dense word embeddings\n",
      "        - Similar optimization techniques (negative sampling, hierarchical softmax)\n",
      "        - The same underlying principle: distributional semantics\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "class Word2VecFramework:\n",
    "    \"\"\"\n",
    "    Word2Vec is the overarching framework containing both Skip-gram and CBOW\n",
    "    \"\"\"\n",
    "    def __init__(self, architecture=\"skip-gram\"):\n",
    "        self.architecture = architecture\n",
    "        print(f\"Word2Vec Framework initialized with {architecture} architecture\")\n",
    "    \n",
    "    def explain_relationship(self):\n",
    "        explanation = \"\"\"\n",
    "        Word2Vec Framework\n",
    "        ├── Skip-gram Model\n",
    "        │   └── Predicts context from target\n",
    "        │       Example: \"fox\" → [\"quick\", \"brown\", \"jumps\", \"over\"]\n",
    "        │\n",
    "        └── CBOW Model\n",
    "            └── Predicts target from context\n",
    "                Example: [\"quick\", \"brown\", \"jumps\", \"over\"] → \"fox\"\n",
    "        \n",
    "        Both models share:\n",
    "        - The goal of learning dense word embeddings\n",
    "        - Similar optimization techniques (negative sampling, hierarchical softmax)\n",
    "        - The same underlying principle: distributional semantics\n",
    "\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        print(explanation)\n",
    "\n",
    "# Demonstrate the relationship\n",
    "word2vec = Word2VecFramework()\n",
    "word2vec.explain_relationship()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "to sum up.\n",
    "Word2Vec comes in two core flavors:\n",
    "\n",
    "- **Skip-gram**:  \n",
    "  Given a **target word**, the model tries to predict its surrounding **context words**.  \n",
    "  This architecture works well with **large, sparse datasets** and is particularly effective at capturing **rare word semantics**.\n",
    "\n",
    "- **Continuous Bag of Words (CBOW)**:  \n",
    "  Given a set of **context words**, the model tries to predict the **target word**.  \n",
    "  This approach is typically faster and works better with **frequent words**.\n",
    "\n",
    "These two approaches share the same goal:\n",
    "➡️ Learn **word vectors** such that words with **similar contexts** are mapped to **similar embeddings** in vector space.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ How Does Word2Vec Work?\n",
    "\n",
    "1. **Training Objective**  \n",
    "   Word2Vec treats word relationships as a **prediction problem**, not a counting problem.  \n",
    "   Using a shallow neural network (typically 1 hidden layer), the model is trained to **maximize the probability** of observed word-context pairs.\n",
    "\n",
    "2. **Input and Output**  \n",
    "   - The **input layer** takes one-hot encoded vectors representing words.  \n",
    "   - The **hidden layer** contains the **learned word embeddings**.  \n",
    "   - The **output layer** predicts probabilities over the vocabulary using a softmax function (or approximations like negative sampling).\n",
    "\n",
    "3. **Optimization**  \n",
    "   Word2Vec uses techniques like:\n",
    "   - **Negative Sampling**: Instead of updating the full softmax over all words, it updates only a small subset of \"negative\" samples.\n",
    "   - **Hierarchical Softmax**: Speeds up training by organizing the vocabulary in a binary tree.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Skip-gram in Action\n",
    "\n",
    "Let’s say your corpus contains the sentence:\n",
    "\n",
    "> \"The cat sat on the mat.\"\n",
    "\n",
    "If the target word is `\"sat\"` and the context window size is 2,  \n",
    "the Skip-gram model learns to predict:\n",
    "\n",
    "- Input: `\"sat\"`  \n",
    "- Output: `\"cat\"`, `\"on\"`\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Why Does This Work?\n",
    "\n",
    "Over time, the network adjusts the word vectors so that words occurring in **similar contexts** end up having **similar vector representations**.  \n",
    "This results in a **semantic space** where analogies like:\n",
    "\n",
    "> *king - man + woman ≈ queen*\n",
    "\n",
    "are **arithmetically meaningful**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌍💵 Real-world Impact\n",
    "\n",
    "Word2Vec revolutionized NLP by making it possible to:\n",
    "\n",
    "- Cluster semantically similar words\n",
    "- Improve text classification and search\n",
    "- Enable analogical reasoning in vector space\n",
    "- Pretrain embeddings for deep learning models\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Part 2: The Skip-gram Architecture\n",
    "\n",
    "### Core Concept: Predicting Context from Target\n",
    "\n",
    "The skip-gram model takes a target word and predicts surrounding context words. Let's break this down step by step.\n",
    "\n",
    "Given the sentence: \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "If our target word is \"fox\" and we use a context window of size 2, we want to predict:\n",
    "- Context words: [\"quick\", \"brown\", \"jumps\", \"over\"]\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The skip-gram model uses two embedding matrices:\n",
    "- **W** ∈ ℝ^(|V|×d): Target word embeddings (input embeddings)\n",
    "- **C** ∈ ℝ^(|V|×d): Context word embeddings (output embeddings)\n",
    "\n",
    "Where:\n",
    "- |V| is vocabulary size\n",
    "- d is embedding dimension\n",
    "\n",
    "For a target word w and context word c, the probability is:\n",
    "\n",
    "P(c|w) = exp(c·w) / Σ(exp(c'·w)) for all c' in V\n",
    "\n",
    "> ⚠️ **Critical Insight**: We maintain TWO sets of embeddings! This separation helps training stability and often produces better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(quick|fox) = 0.000100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SkipGramDemo:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        # Initialize two embedding matrices\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * 0.01  # Target embeddings\n",
    "        self.C = np.random.randn(vocab_size, embedding_dim) * 0.01  # Context embeddings\n",
    "        \n",
    "    def compute_probability(self, target_idx, context_idx):\n",
    "        \"\"\"\n",
    "        Compute P(context|target) using softmax\n",
    "        \n",
    "        Args:\n",
    "            target_idx: Index of target word\n",
    "            context_idx: Index of context word\n",
    "        \n",
    "        Returns:\n",
    "            probability: P(context|target)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        target_embedding = self.W[target_idx]\n",
    "        context_embedding = self.C[context_idx]\n",
    "        \n",
    "        # Compute dot product\n",
    "        score = np.dot(target_embedding, context_embedding)\n",
    "        \n",
    "        # Compute softmax denominator (sum over all context words)\n",
    "        all_scores = np.dot(self.C, target_embedding)\n",
    "        exp_scores = np.exp(all_scores - np.max(all_scores))  # Numerical stability\n",
    "        \n",
    "        probability = exp_scores[context_idx] / np.sum(exp_scores)\n",
    "        return probability\n",
    "\n",
    "# Demonstration\n",
    "vocab_size, embedding_dim = 10000, 100\n",
    "model = SkipGramDemo(vocab_size, embedding_dim)\n",
    "\n",
    "# Example computation\n",
    "target_word_idx = 42  # Index for \"fox\"\n",
    "context_word_idx = 156  # Index for \"quick\"\n",
    "prob = model.compute_probability(target_word_idx, context_word_idx)\n",
    "print(f\"P(quick|fox) = {prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Computational Challenge: Softmax\n",
    "\n",
    "The softmax computation requires summing over the entire vocabulary, making it computationally expensive for large vocabularies. This leads us to...\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Negative Sampling - Making Word2Vec Practical\n",
    "\n",
    "### The Problem with Full Softmax\n",
    "\n",
    "Computing the softmax denominator requires O(|V|) operations per training example. With vocabularies often exceeding 100,000 words, this becomes prohibitively expensive.\n",
    "\n",
    "### Solution: Negative Sampling\n",
    "\n",
    "Instead of updating all word vectors for each training example, negative sampling:\n",
    "1. Updates the target word vector\n",
    "2. Updates the positive context word vector\n",
    "3. Updates only k randomly selected \"negative\" context word vectors\n",
    "\n",
    "The new objective function becomes:\n",
    "\n",
    "log σ(c_pos·w) + Σ[k negative samples] log σ(-c_neg·w)\n",
    "\n",
    "Where σ(x) = 1/(1+exp(-x)) is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "def sigmoid(x):\n",
    "    \"\"\"Numerically stable sigmoid function\"\"\"\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "class SkipGramNegativeSampling:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.C = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def train_pair(self, target_idx, context_idx, negative_samples, learning_rate=0.025):\n",
    "        \"\"\"\n",
    "        Train on one target-context pair with negative sampling\n",
    "        \n",
    "        Args:\n",
    "            target_idx: Target word index\n",
    "            context_idx: Positive context word index\n",
    "            negative_samples: List of negative sample indices\n",
    "            learning_rate: Learning rate for SGD\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        target_vec = self.W[target_idx]\n",
    "        context_vec = self.C[context_idx]\n",
    "        \n",
    "        # Positive example gradient\n",
    "        score = np.dot(target_vec, context_vec)\n",
    "        sigmoid_score = sigmoid(score)\n",
    "        grad_target = (sigmoid_score - 1) * context_vec\n",
    "        grad_context = (sigmoid_score - 1) * target_vec\n",
    "        \n",
    "        # Update for positive example\n",
    "        self.W[target_idx] -= learning_rate * grad_target\n",
    "        self.C[context_idx] -= learning_rate * grad_context\n",
    "        \n",
    "        # Negative examples\n",
    "        for neg_idx in negative_samples:\n",
    "            neg_vec = self.C[neg_idx]\n",
    "            score = np.dot(target_vec, neg_vec)\n",
    "            sigmoid_score = sigmoid(score)\n",
    "            \n",
    "            # Gradients for negative examples (note the sign difference)\n",
    "            grad_target_neg = sigmoid_score * neg_vec\n",
    "            grad_neg = sigmoid_score * target_vec\n",
    "            \n",
    "            # Update\n",
    "            self.W[target_idx] -= learning_rate * grad_target_neg\n",
    "            self.C[neg_idx] -= learning_rate * grad_neg\n",
    "\n",
    "# Example usage\n",
    "model = SkipGramNegativeSampling(vocab_size=10000, embedding_dim=100)\n",
    "target_idx = 42\n",
    "context_idx = 156\n",
    "negative_samples = np.random.randint(0, 10000, size=5)\n",
    "model.train_pair(target_idx, context_idx, negative_samples)\n",
    "```\n",
    "\n",
    "### Choosing Negative Samples\n",
    "\n",
    "Word2Vec doesn't sample negatives uniformly. Instead, it uses:\n",
    "\n",
    "P(w) = count(w)^(3/4) / Σ count(w')^(3/4)\n",
    "\n",
    "This formula raises word frequencies to the 3/4 power, which:\n",
    "- Increases probability of selecting rare words\n",
    "- Decreases probability of selecting very common words\n",
    "- Results in better embeddings overall\n",
    "\n",
    "```python\n",
    "def create_negative_sampling_distribution(word_counts, power=0.75):\n",
    "    \"\"\"\n",
    "    Create distribution for negative sampling\n",
    "    \n",
    "    Args:\n",
    "        word_counts: Dictionary mapping word indices to their counts\n",
    "        power: Power to raise frequencies (typically 0.75)\n",
    "    \n",
    "    Returns:\n",
    "        sampling_probs: Array of sampling probabilities\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_counts)\n",
    "    counts = np.array([word_counts.get(i, 0) for i in range(vocab_size)])\n",
    "    \n",
    "    # Raise to power and normalize\n",
    "    powered_counts = counts ** power\n",
    "    sampling_probs = powered_counts / np.sum(powered_counts)\n",
    "    \n",
    "    return sampling_probs\n",
    "\n",
    "# Example\n",
    "word_counts = {i: np.random.zipf(1.5) for i in range(1000)}\n",
    "sampling_dist = create_negative_sampling_distribution(word_counts)\n",
    "\n",
    "# Visualize the effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "original_probs = np.array(list(word_counts.values())) / sum(word_counts.values())\n",
    "plt.scatter(original_probs, sampling_dist, alpha=0.5)\n",
    "plt.xlabel('Original Probability')\n",
    "plt.ylabel('Negative Sampling Probability')\n",
    "plt.title('Effect of Power Scaling on Sampling Distribution')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot([1e-5, 1e-1], [1e-5, 1e-1], 'r--', label='y=x')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Training Word2Vec - The Complete Algorithm\n",
    "\n",
    "Let's put everything together to understand the full training process:\n",
    "\n",
    "```python\n",
    "class Word2Vec:\n",
    "    def __init__(self, sentences, embedding_dim=100, window_size=5, \n",
    "                 negative_samples=5, epochs=5, learning_rate=0.025):\n",
    "        # Build vocabulary\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.word_counts = {}\n",
    "        self.build_vocabulary(sentences)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.vocab_size = len(self.word_to_idx)\n",
    "        self.W = np.random.randn(self.vocab_size, embedding_dim) * 0.01\n",
    "        self.C = np.random.randn(self.vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # Training parameters\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.epochs = epochs\n",
    "        self.initial_lr = learning_rate\n",
    "        \n",
    "        # Create negative sampling distribution\n",
    "        self.create_sampling_table()\n",
    "        \n",
    "    def build_vocabulary(self, sentences):\n",
    "        \"\"\"Build vocabulary from sentences\"\"\"\n",
    "        idx = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.word_to_idx[word] = idx\n",
    "                    self.idx_to_word[idx] = word\n",
    "                    self.word_counts[idx] = 0\n",
    "                    idx += 1\n",
    "                self.word_counts[self.word_to_idx[word]] += 1\n",
    "    \n",
    "    def create_sampling_table(self):\n",
    "        \"\"\"Create table for negative sampling\"\"\"\n",
    "        power = 0.75\n",
    "        norm = sum([count**power for count in self.word_counts.values()])\n",
    "        \n",
    "        table_size = 1e8\n",
    "        table = []\n",
    "        \n",
    "        pow_frequency = [count**power / norm for count in self.word_counts.values()]\n",
    "        word_index = 0\n",
    "        total_pow_freq = pow_frequency[word_index]\n",
    "        \n",
    "        for i in range(int(table_size)):\n",
    "            table.append(word_index)\n",
    "            if i / table_size > total_pow_freq:\n",
    "                word_index += 1\n",
    "                if word_index < len(self.word_counts):\n",
    "                    total_pow_freq += pow_frequency[word_index]\n",
    "                else:\n",
    "                    word_index = len(self.word_counts) - 1\n",
    "        \n",
    "        self.sampling_table = np.array(table)\n",
    "    \n",
    "    def get_negative_samples(self, target_idx, context_idx):\n",
    "        \"\"\"Get negative samples, avoiding target and context\"\"\"\n",
    "        negative_samples = []\n",
    "        while len(negative_samples) < self.negative_samples:\n",
    "            neg_idx = self.sampling_table[np.random.randint(len(self.sampling_table))]\n",
    "            if neg_idx != target_idx and neg_idx != context_idx:\n",
    "                negative_samples.append(neg_idx)\n",
    "        return negative_samples\n",
    "    \n",
    "    def train_pair(self, target_idx, context_idx, learning_rate):\n",
    "        \"\"\"Train on a single target-context pair\"\"\"\n",
    "        negative_samples = self.get_negative_samples(target_idx, context_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        target_vec = self.W[target_idx]\n",
    "        context_vec = self.C[context_idx]\n",
    "        \n",
    "        # Positive example\n",
    "        score = np.dot(target_vec, context_vec)\n",
    "        sigmoid_score = sigmoid(score)\n",
    "        \n",
    "        # Gradients\n",
    "        grad_target = (sigmoid_score - 1) * context_vec\n",
    "        grad_context = (sigmoid_score - 1) * target_vec\n",
    "        \n",
    "        # Negative examples\n",
    "        for neg_idx in negative_samples:\n",
    "            neg_vec = self.C[neg_idx]\n",
    "            neg_score = np.dot(target_vec, neg_vec)\n",
    "            neg_sigmoid = sigmoid(neg_score)\n",
    "            \n",
    "            grad_target += neg_sigmoid * neg_vec\n",
    "            self.C[neg_idx] -= learning_rate * neg_sigmoid * target_vec\n",
    "        \n",
    "        # Update embeddings\n",
    "        self.W[target_idx] -= learning_rate * grad_target\n",
    "        self.C[context_idx] -= learning_rate * grad_context\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"Train Word2Vec model\"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            word_count = 0\n",
    "            \n",
    "            # Linearly decay learning rate\n",
    "            lr = self.initial_lr * (1 - epoch / self.epochs)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                sentence_indices = [self.word_to_idx[word] for word in sentence]\n",
    "                \n",
    "                for center_pos, center_idx in enumerate(sentence_indices):\n",
    "                    # Get context window\n",
    "                    context_start = max(0, center_pos - self.window_size)\n",
    "                    context_end = min(len(sentence_indices), center_pos + self.window_size + 1)\n",
    "                    \n",
    "                    for context_pos in range(context_start, context_end):\n",
    "                        if context_pos != center_pos:\n",
    "                            context_idx = sentence_indices[context_pos]\n",
    "                            self.train_pair(center_idx, context_idx, lr)\n",
    "                            word_count += 1\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} completed\")\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Get embedding for a word\"\"\"\n",
    "        if word in self.word_to_idx:\n",
    "            idx = self.word_to_idx[word]\n",
    "            return self.W[idx]  # Usually just use target embeddings\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def most_similar(self, word, top_k=5):\n",
    "        \"\"\"Find most similar words\"\"\"\n",
    "        if word not in self.word_to_idx:\n",
    "            return []\n",
    "        \n",
    "        word_vec = self.get_embedding(word)\n",
    "        similarities = np.dot(self.W, word_vec)\n",
    "        most_similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Exclude the word itself\n",
    "        \n",
    "        return [(self.idx_to_word[idx], similarities[idx]) for idx in most_similar_indices]\n",
    "\n",
    "# Example usage with sample data\n",
    "sample_sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"the\", \"dog\", \"barks\", \"at\", \"the\", \"cat\"],\n",
    "    [\"the\", \"cat\", \"meows\", \"at\", \"the\", \"dog\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(sample_sentences, embedding_dim=50, window_size=2, epochs=5)\n",
    "model.train(sample_sentences)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.most_similar(\"dog\", top_k=3)\n",
    "print(\"Words similar to 'dog':\", similar_words)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Understanding What Word2Vec Learns\n",
    "\n",
    "### Semantic Relationships\n",
    "\n",
    "Word2Vec famously captures semantic relationships through vector arithmetic:\n",
    "\n",
    "```python\n",
    "def demonstrate_word_arithmetic(model, equation):\n",
    "    \"\"\"\n",
    "    Demonstrate word arithmetic like \"king - man + woman = queen\"\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        equation: String like \"king - man + woman\"\n",
    "    \"\"\"\n",
    "    words = equation.split()\n",
    "    result_vec = np.zeros_like(model.W[0])\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in ['+', '-', '=']:\n",
    "            continue\n",
    "        \n",
    "        vec = model.get_embedding(word)\n",
    "        if vec is None:\n",
    "            print(f\"Word '{word}' not in vocabulary\")\n",
    "            return\n",
    "        \n",
    "        if i > 0 and words[i-1] == '-':\n",
    "            result_vec -= vec\n",
    "        else:\n",
    "            result_vec += vec\n",
    "    \n",
    "    # Find closest word to result vector\n",
    "    similarities = np.dot(model.W, result_vec)\n",
    "    most_similar_idx = np.argmax(similarities)\n",
    "    \n",
    "    return model.idx_to_word[most_similar_idx]\n",
    "\n",
    "# Example (with a properly trained model)\n",
    "# result = demonstrate_word_arithmetic(model, \"king - man + woman\")\n",
    "# print(f\"king - man + woman = {result}\")  # Should output something close to \"queen\"\n",
    "```\n",
    "\n",
    "### Visualizing Embeddings\n",
    "\n",
    "We can use dimensionality reduction to visualize high-dimensional embeddings:\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings(model, words_to_plot=None, perplexity=30):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings using t-SNE\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Word2Vec model\n",
    "        words_to_plot: List of words to visualize (None = all words)\n",
    "        perplexity: t-SNE perplexity parameter\n",
    "    \"\"\"\n",
    "    if words_to_plot is None:\n",
    "        words_to_plot = list(model.word_to_idx.keys())\n",
    "    \n",
    "    # Get embeddings for selected words\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for word in words_to_plot:\n",
    "        if word in model.word_to_idx:\n",
    "            embeddings.append(model.get_embedding(word))\n",
    "            words.append(word)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Reduce to 2D using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    xytext=(5, 2), textcoords='offset points')\n",
    "    \n",
    "    plt.title('Word Embeddings Visualization (t-SNE)')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Example visualization\n",
    "# visualize_embeddings(model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Advanced Concepts and Optimizations\n",
    "\n",
    "### Subsampling Frequent Words\n",
    "\n",
    "Very frequent words like \"the\", \"a\", \"is\" provide less information value. Word2Vec uses subsampling to address this:\n",
    "\n",
    "```python\n",
    "def subsample_probability(word_freq, threshold=1e-3):\n",
    "    \"\"\"\n",
    "    Calculate probability of keeping a word during training\n",
    "    \n",
    "    Args:\n",
    "        word_freq: Frequency of the word in corpus\n",
    "        threshold: Subsampling threshold (typically 1e-3 to 1e-5)\n",
    "    \n",
    "    Returns:\n",
    "        probability: Probability of keeping the word\n",
    "    \"\"\"\n",
    "    if word_freq <= 0:\n",
    "        return 0\n",
    "    \n",
    "    prob = 1 - np.sqrt(threshold / word_freq)\n",
    "    return max(0, prob)\n",
    "\n",
    "# Visualization of subsampling effect\n",
    "frequencies = np.logspace(-6, -1, 100)\n",
    "keep_probs = [1 - subsample_probability(f) for f in frequencies]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(frequencies, keep_probs)\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Probability of Keeping Word')\n",
    "plt.title('Effect of Subsampling on Different Word Frequencies')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Hierarchical Softmax (Alternative to Negative Sampling)\n",
    "\n",
    "Hierarchical softmax organizes words in a binary tree, reducing complexity from O(|V|) to O(log|V|):\n",
    "\n",
    "```python\n",
    "class HierarchicalSoftmaxNode:\n",
    "    def __init__(self, left=None, right=None, word_idx=None, vector=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.word_idx = word_idx\n",
    "        self.vector = vector if vector is not None else np.random.randn(embedding_dim) * 0.01\n",
    "\n",
    "def build_huffman_tree(word_freqs):\n",
    "    \"\"\"\n",
    "    Build Huffman tree for hierarchical softmax\n",
    "    \n",
    "    Args:\n",
    "        word_freqs: Dictionary of word frequencies\n",
    "    \n",
    "    Returns:\n",
    "        root: Root node of Huffman tree\n",
    "    \"\"\"\n",
    "    import heapq\n",
    "    \n",
    "    # Create leaf nodes\n",
    "    heap = []\n",
    "    for word_idx, freq in word_freqs.items():\n",
    "        node = HierarchicalSoftmaxNode(word_idx=word_idx)\n",
    "        heapq.heappush(heap, (freq, id(node), node))\n",
    "    \n",
    "    # Build tree\n",
    "    while len(heap) > 1:\n",
    "        freq1, _, node1 = heapq.heappop(heap)\n",
    "        freq2, _, node2 = heapq.heappop(heap)\n",
    "        \n",
    "        parent = HierarchicalSoftmaxNode(left=node1, right=node2)\n",
    "        heapq.heappush(heap, (freq1 + freq2, id(parent), parent))\n",
    "    \n",
    "    _, _, root = heapq.heappop(heap)\n",
    "    return root\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part 7: Quiz Time! 🧠\n",
    "\n",
    "Let's test your understanding with some questions:\n",
    "\n",
    "### Question 1: Core Concepts\n",
    "What is the fundamental difference between Word2Vec and traditional sparse representations?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "Word2Vec creates dense, continuous vectors that capture semantic relationships, while sparse representations use one-hot or count-based vectors that don't inherently encode semantic similarity. Word2Vec learns these representations by predicting context words, making similar words have similar vectors.\n",
    "</details>\n",
    "\n",
    "### Question 2: Architecture\n",
    "In skip-gram, why do we maintain two separate embedding matrices (W for targets, C for contexts)?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "Having separate matrices:\n",
    "1. Provides more flexibility in learning\n",
    "2. Helps avoid trivial solutions where a word predicts itself\n",
    "3. Often leads to better quality embeddings\n",
    "4. Allows different roles for words as targets vs. contexts\n",
    "</details>\n",
    "\n",
    "### Question 3: Negative Sampling\n",
    "Why do we raise word frequencies to the power of 3/4 in negative sampling instead of using uniform or raw frequency distributions?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "The 3/4 power:\n",
    "1. Increases the probability of sampling rare words (compared to their actual frequency)\n",
    "2. Decreases the probability of sampling very common words\n",
    "3. Results in a more balanced training signal\n",
    "4. Leads to better quality embeddings for both common and rare words\n",
    "</details>\n",
    "\n",
    "### Question 4: Practical Implementation\n",
    "What's the time complexity of:\n",
    "a) Full softmax computation?\n",
    "b) Negative sampling with k negative samples?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answer</summary>\n",
    "\n",
    "a) Full softmax: O(|V|) where |V| is vocabulary size\n",
    "b) Negative sampling: O(k) where k is typically 5-20\n",
    "\n",
    "This is why negative sampling makes Word2Vec practical for large vocabularies!\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## Part 8: Practical Tips for Training Word2Vec\n",
    "\n",
    "1. **Preprocessing is crucial**:\n",
    "   - Lowercase text\n",
    "   - Remove very rare words (appearing < 5 times)\n",
    "   - Consider replacing numbers with a special token\n",
    "\n",
    "2. **Hyperparameter guidelines**:\n",
    "   - Embedding dimension: 100-300 (more dimensions for larger datasets)\n",
    "   - Window size: 5-10 (smaller for syntactic tasks, larger for semantic tasks)\n",
    "   - Negative samples: 5-20 (more for larger datasets)\n",
    "   - Learning rate: Start at 0.025 with linear decay\n",
    "\n",
    "3. **Training corpus size**:\n",
    "   - Minimum: 100k words\n",
    "   - Better results: 1M+ words\n",
    "   - State-of-the-art: Billions of words\n",
    "\n",
    "4. **Evaluation during training**:\n",
    "   - Monitor analogy tasks\n",
    "   - Check nearest neighbors for common words\n",
    "   - Use intrinsic evaluation sets like WordSim-353\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion: The Impact of Word2Vec\n",
    "\n",
    "Word2Vec revolutionized NLP by showing that:\n",
    "1. Simple prediction tasks can learn rich semantic representations\n",
    "2. Dense embeddings outperform sparse representations\n",
    "3. Word meanings can be captured through their contexts\n",
    "4. Neural approaches can scale to massive datasets\n",
    "\n",
    "These insights paved the way for modern transformer models and the current revolution in NLP!\n",
    "\n",
    "### Further Resources 📚\n",
    "\n",
    "1. **Original Papers**:\n",
    "   - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) (Mikolov et al., 2013)\n",
    "   - [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546) (Mikolov et al., 2013)\n",
    "\n",
    "2. **Implementations**:\n",
    "   - [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)\n",
    "   - [TensorFlow Word2Vec Tutorial](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "\n",
    "3. **Advanced Topics**:\n",
    "   - [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "   - [FastText: Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "\n",
    "4. **Visualization Tools**:\n",
    "   - [Embedding Projector](https://projector.tensorflow.org/)\n",
    "   - [Word2Vec Demo](https://ronxin.github.io/wevi/)\n",
    "\n",
    "Keep practicing and experimenting with these concepts! The journey from Word2Vec to modern language models is fascinating and builds directly on these foundations. 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-journey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
