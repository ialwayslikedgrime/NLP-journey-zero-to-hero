{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Word2Vec Mathematics\n",
    "\n",
    "\n",
    "\n",
    "### The Big Picture: What Are We Computing?\n",
    "\n",
    "Before diving into formulas, let's establish a mental framework. Word2Vec is essentially asking: \"How likely is it that two words appear together?\" But instead of counting, we're predicting - and this prediction process teaches our model the meaning of words.\n",
    "\n",
    "### The Fundamental Question\n",
    "\n",
    "Given a target word `w` and a potential context word `c`, we want to compute:\n",
    "\n",
    "**P(+|w,c)** = Probability that c is actually a context word for w\n",
    "\n",
    "Think of this like your brain recognizing patterns. When you see \"coffee,\" your brain automatically activates related concepts like \"cup,\" \"hot,\" and \"morning.\" Word2Vec mimics this neural activation pattern.\n",
    "\n",
    "## Step 1: From Words to Numbers (Embeddings)\n",
    "\n",
    "First, we need to convert words into vectors. This is like how your brain encodes concepts - each word gets a unique pattern of activation across neurons.\n",
    "\n",
    "```\n",
    "Word \"apple\" â†’ Vector [0.2, -0.5, 0.8, ...]\n",
    "Word \"fruit\" â†’ Vector [0.3, -0.4, 0.7, ...]\n",
    "```\n",
    "\n",
    "**Neuroscience Hack**: Visualize each dimension as a different \"neural feature detector.\" One dimension might respond to \"edibility,\" another to \"color,\" etc.\n",
    "\n",
    "## Step 2: Measuring Word Similarity\n",
    "\n",
    "The key insight is using the **dot product** to measure similarity:\n",
    "\n",
    "**Similarity(w,c) â‰ˆ cÂ·w**\n",
    "\n",
    "Let's break this down:\n",
    "- If vectors point in similar directions â†’ large positive dot product\n",
    "- If vectors are perpendicular â†’ dot product near zero\n",
    "- If vectors point in opposite directions â†’ negative dot product\n",
    "\n",
    "**Visual Analogy**: Imagine two flashlight beams. The dot product measures how much they overlap:\n",
    "- Parallel beams = maximum overlap (high similarity)\n",
    "- Perpendicular beams = no overlap (no relationship)\n",
    "- Opposite directions = negative overlap (opposite meanings)\n",
    "\n",
    "## Step 3: Converting Similarity to Probability\n",
    "\n",
    "The dot product gives us a number from -âˆž to +âˆž, but we need a probability (0 to 1). Enter the **sigmoid function**:\n",
    "\n",
    "**Ïƒ(x) = 1 / (1 + exp(-x))**\n",
    "\n",
    "**Neuroscience Hack**: The sigmoid is exactly how biological neurons work! They convert continuous input into a firing probability. Here's what happens:\n",
    "- Large positive input (x >> 0) â†’ Ïƒ(x) â‰ˆ 1 (neuron fires)\n",
    "- Zero input (x = 0) â†’ Ïƒ(x) = 0.5 (uncertain)\n",
    "- Large negative input (x << 0) â†’ Ïƒ(x) â‰ˆ 0 (neuron silent)\n",
    "\n",
    "So our probability becomes:\n",
    "\n",
    "**P(+|w,c) = Ïƒ(cÂ·w) = 1 / (1 + exp(-cÂ·w))**\n",
    "\n",
    "## Step 4: The Learning Process\n",
    "\n",
    "Word2Vec learns by adjusting vectors to increase P(+|w,c) for actual context pairs and decrease it for random pairs.\n",
    "\n",
    "### The Loss Function\n",
    "\n",
    "The loss function is what we minimize during training:\n",
    "\n",
    "**L = -log Ïƒ(cposÂ·w) + Î£ log Ïƒ(-cnegÂ·w)**\n",
    "\n",
    "Let's decode this:\n",
    "1. **-log Ïƒ(cposÂ·w)**: We want to maximize probability for real context words\n",
    "   - Taking -log converts \"maximize probability\" to \"minimize loss\"\n",
    "   - When probability is high (near 1), -log is small (near 0)\n",
    "   \n",
    "2. **Î£ log Ïƒ(-cnegÂ·w)**: We want to minimize probability for random words\n",
    "   - The negative sign in (-cnegÂ·w) flips our similarity measure\n",
    "   - We sum over k negative samples\n",
    "\n",
    "**Memory Trick**: Think \"PUSH-PULL\"\n",
    "- PUSH real context words closer (minimize first term)\n",
    "- PULL random words away (minimize second term)\n",
    "\n",
    "## Step 5: Gradient Descent Updates\n",
    "\n",
    "The update rules tell us how to adjust our vectors:\n",
    "\n",
    "### For positive context word:\n",
    "**âˆ‚L/âˆ‚cpos = [Ïƒ(cposÂ·w) - 1]w**\n",
    "\n",
    "**Intuition**: \n",
    "- If Ïƒ(cposÂ·w) â‰ˆ 1 (already good), gradient â‰ˆ 0 (small update)\n",
    "- If Ïƒ(cposÂ·w) â‰ˆ 0 (bad), gradient â‰ˆ -w (big update toward w)\n",
    "\n",
    "### For negative samples:\n",
    "**âˆ‚L/âˆ‚cneg = [Ïƒ(cnegÂ·w)]w**\n",
    "\n",
    "**Intuition**:\n",
    "- If Ïƒ(cnegÂ·w) â‰ˆ 0 (already good), gradient â‰ˆ 0 (small update)\n",
    "- If Ïƒ(cnegÂ·w) â‰ˆ 1 (bad), gradient â‰ˆ w (big update away from w)\n",
    "\n",
    "### The Update Rules:\n",
    "```\n",
    "cpos(t+1) = cpos(t) - Î·[Ïƒ(cposÂ·w) - 1]w\n",
    "cneg(t+1) = cneg(t) - Î·[Ïƒ(cnegÂ·w)]w\n",
    "w(t+1) = w(t) - Î·{[Ïƒ(cposÂ·w) - 1]cpos + Î£[Ïƒ(cnegÂ·w)]cneg}\n",
    "```\n",
    "\n",
    "Where Î· (eta) is the learning rate - think of it as \"step size\" in learning.\n",
    "\n",
    "## Negative Sampling: The Efficiency Trick\n",
    "\n",
    "Instead of computing probabilities over all words (expensive!), we:\n",
    "1. Take one positive example\n",
    "2. Sample k negative examples\n",
    "3. Update only these k+1 vectors\n",
    "\n",
    "The sampling probability uses a special formula:\n",
    "\n",
    "**PÎ±(w) = count(w)^Î± / Î£ count(w')^Î±**\n",
    "\n",
    "With Î± = 0.75, this:\n",
    "- Boosts rare words (gives them more training)\n",
    "- Reduces very common words (they already get enough training)\n",
    "\n",
    "**Brain Analogy**: Your brain pays more attention to unusual events (rare words) than everyday occurrences (common words).\n",
    "\n",
    "## Quiz Time! ðŸ§ \n",
    "\n",
    "### Question 1: Dot Product Intuition\n",
    "If two word vectors have a dot product of -5, what does this suggest about their relationship?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "A dot product of -5 (large negative value) suggests the words have opposite or contrasting meanings. They point in nearly opposite directions in the vector space, indicating semantic opposition (like \"hot\" and \"cold\").\n",
    "</details>\n",
    "\n",
    "### Question 2: Sigmoid Function\n",
    "What is Ïƒ(0), and what does this value mean in the context of Word2Vec?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Ïƒ(0) = 1/(1+exp(0)) = 1/(1+1) = 0.5\n",
    "\n",
    "This means when the dot product between two word vectors is 0 (they're perpendicular/unrelated), the probability that one is a context word for the other is exactly 0.5 - complete uncertainty.\n",
    "</details>\n",
    "\n",
    "### Question 3: Gradient Understanding\n",
    "If Ïƒ(cposÂ·w) = 0.9 for a positive example, will the gradient update be large or small? Why?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "The gradient will be small. The gradient is [Ïƒ(cposÂ·w) - 1] = [0.9 - 1] = -0.1.\n",
    "\n",
    "Since we're already close to the target (probability near 1), we only need a small adjustment. This prevents overshooting and helps convergence.\n",
    "</details>\n",
    "\n",
    "### Question 4: Negative Sampling\n",
    "Why do we use count(w)^0.75 instead of just count(w) for negative sampling?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "Using count(w)^0.75 makes the distribution more uniform:\n",
    "- It increases the relative probability of sampling rare words\n",
    "- It decreases the relative probability of sampling very common words\n",
    "- This gives rare words more training opportunities and prevents common words from dominating the negative samples\n",
    "</details>\n",
    "\n",
    "### Question 5: Loss Function Interpretation\n",
    "In the loss function L = -log Ïƒ(cposÂ·w) + Î£ log Ïƒ(-cnegÂ·w), why do we use -cnegÂ·w (with a negative sign) in the second term?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answer</summary>\n",
    "\n",
    "The negative sign flips the similarity measure. We want:\n",
    "- High probability for positive pairs: Ïƒ(cposÂ·w) â†’ 1\n",
    "- Low probability for negative pairs: Ïƒ(cnegÂ·w) â†’ 0\n",
    "\n",
    "Using -cnegÂ·w in Ïƒ(-cnegÂ·w) effectively computes 1 - Ïƒ(cnegÂ·w), giving us the probability that cneg is NOT a context word. We want this to be high (near 1) for negative samples.\n",
    "</details>\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Dot products measure semantic similarity** - the foundation of Word2Vec\n",
    "2. **Sigmoid converts similarities to probabilities** - just like neural activation\n",
    "3. **The loss function implements \"attract-repel\"** - pull related words together, push unrelated apart\n",
    "4. **Negative sampling makes training efficient** - update only a few vectors per step\n",
    "5. **Gradients guide learning** - bigger errors lead to bigger updates\n",
    "\n",
    "Remember: Word2Vec is teaching a neural network to predict context, and the side effect is learning meaningful word representations!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
