{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Natural Language Processing - Vector Semantics and Embeddings\n",
    "\n",
    "## üéØ Goals of this notebook\n",
    "- Understand the basics of vector semantics\n",
    "- Learn the difference between sparse and dense vector representations\n",
    "- Master key concepts like term-document matrices, word-context matrices, and weighting schemes\n",
    "- Explore Word2Vec and how neural embeddings work\n",
    "- Implement simple code examples for these concepts\n",
    "- Apply and test your knowledge through interactive exercises\n",
    "\n",
    "## 1. Introduction to Vector Semantics\n",
    "\n",
    "### üìñ **What is Vector Semantics?**\n",
    "\n",
    "Vector semantics is a powerful way to represent word meaning in natural language processing. The core idea is surprisingly simple yet profound:\n",
    "\n",
    "> A word's meaning can be represented as a point in a multi-dimensional space - a **vector**.\n",
    "\n",
    "This approach, also called the **distributional hypothesis**, is based on the intuition that words with similar meanings tend to occur in similar contexts. As linguist J.R. Firth famously said in 1957:\n",
    "\n",
    "> \"You shall know a word by the company it keeps.\"\n",
    "\n",
    "For example, the words \"cat\" and \"dog\" often appear in similar contexts (pet, animal, fur, etc.), so their vector representations should be close to each other in our semantic space.\n",
    "\n",
    "### üí° Curiosity: From Philosophy to Computation\n",
    "\n",
    "The idea that \"meaning is use\" actually has philosophical roots in Ludwig Wittgenstein's later work. Rather than trying to define words with formal logical definitions, Wittgenstein suggested that \"the meaning of a word is its use in the language.\" This philosophical insight eventually led to computational approaches to semantics!\n",
    "\n",
    "### üìñ **Why Vectors for Words?**\n",
    "\n",
    "Why represent words as vectors? There are several advantages:\n",
    "\n",
    "1. **Quantifiable similarity**: We can measure how similar two words are by measuring the distance between their vectors\n",
    "2. **Continuous representation**: Instead of discrete, binary relationships, we get graduated similarities\n",
    "3. **Automatic learning**: These relationships can be learned from large text corpora\n",
    "4. **Computation-friendly**: Vector operations are efficient and well-supported\n",
    "\n",
    "### üß™ Try it yourself: Word similarity\n",
    "\n",
    "Think about these word pairs. Which ones seem most similar in meaning?\n",
    "- cat / dog\n",
    "- cat / table\n",
    "- run / sprint\n",
    "- happy / sad\n",
    "- happy / joyful\n",
    "\n",
    "If you thought cat/dog, run/sprint, and happy/joyful were the most similar pairs, you're already thinking like a vector semantics system! Later, we'll see how our algorithms capture these similarities.\n",
    "\n",
    "## 2. From Words to Vectors: The Basics\n",
    "\n",
    "### üìñ **Two Types of Vector Representations**\n",
    "\n",
    "There are two main types of vector representations for words:\n",
    "\n",
    "1. **Sparse vectors**: Very high-dimensional with mostly zeros\n",
    "   - Dimensions often = vocabulary size (tens of thousands)\n",
    "   - Each dimension has a clear interpretation\n",
    "   - Examples: one-hot vectors, tf-idf, PPMI matrices\n",
    "\n",
    "2. **Dense vectors**: Lower-dimensional with mostly non-zero values\n",
    "   - Dimensions typically range from 50-1000\n",
    "   - Dimensions don't have clear interpretations\n",
    "   - Examples: word2vec, GloVe, fastText\n",
    "\n",
    "### üìñ **Representing Words in Vector Space**\n",
    "\n",
    "Let's start with the simplest approach to understand how we might represent words as vectors.\n",
    "\n",
    "#### One-Hot Encoding\n",
    "\n",
    "The simplest vector representation is a **one-hot vector**:\n",
    "- Create a vector with length = vocabulary size\n",
    "- Set element i to 1 if it corresponds to word i\n",
    "- Set all other elements to 0\n",
    "\n",
    "For example, with vocabulary [cat, dog, house, book, run]:\n",
    "\n",
    "- \"cat\" = [1, 0, 0, 0, 0]\n",
    "- \"dog\" = [0, 1, 0, 0, 0]\n",
    "- \"book\" = [0, 0, 0, 1, 0]\n",
    "\n",
    "### ‚ö° Challenge: One-Hot Encoding Limitation\n",
    "\n",
    "Can you think of a major limitation of one-hot encoding for representing word meaning?\n",
    "\n",
    "<details>\n",
    "<summary>Click to see the answer</summary>\n",
    "\n",
    "One-hot encoding treats all words as equally different from each other! The distance between any two different words is exactly the same, so we don't capture any notion of semantic similarity.\n",
    "\n",
    "For example, \"cat\" and \"dog\" are just as different as \"cat\" and \"algorithm\" according to one-hot encoding, which doesn't match our intuition about meaning.\n",
    "</details>\n",
    "\n",
    "### üìñ **Building Better Vectors: Co-occurrence Counts**\n",
    "\n",
    "To capture similarity, we need to look at how words are used in context. A simple but powerful approach is to count **co-occurrences** - how often words appear near each other in text.\n",
    "\n",
    "There are two main types of co-occurrence matrices:\n",
    "\n",
    "1. **Term-Document Matrix**: Rows = words, Columns = documents\n",
    "2. **Term-Term Matrix** (or Word-Context Matrix): Rows = target words, Columns = context words\n",
    "\n",
    "Let's look at each one!\n",
    "\n",
    "## 3. Term-Document Matrices\n",
    "\n",
    "### üìñ **The Term-Document Matrix**\n",
    "\n",
    "A term-document matrix represents words based on which documents they appear in:\n",
    "- Each row represents a word (term)\n",
    "- Each column represents a document\n",
    "- Each cell value shows how many times that word occurs in that document\n",
    "\n",
    "For example, with documents about \"pets\", \"cooking\", and \"sports\":\n",
    "\n",
    "|          | doc_pets | doc_cooking | doc_sports |\n",
    "|----------|----------|-------------|------------|\n",
    "| cat      | 5        | 0           | 0          |\n",
    "| dog      | 7        | 0           | 0          |\n",
    "| flour    | 0        | 4           | 0          |\n",
    "| recipe   | 0        | 6           | 0          |\n",
    "| ball     | 1        | 0           | 8          |\n",
    "| team     | 0        | 0           | 5          |\n",
    "\n",
    "Notice how words that appear in similar documents will have similar vectors. \"Cat\" and \"dog\" both appear in the pets document, making their vectors similar.\n",
    "\n",
    "### üíª Code section: Creating a Term-Document Matrix\n",
    "\n",
    "Let's create a simple term-document matrix from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 33\n",
      "\n",
      "Term-Document Matrix:\n",
      "            doc_1  doc_2  doc_3  doc_4  doc_5  doc_6  doc_7\n",
      "and           1.0    0.0    0.0    1.0    1.0    0.0    0.0\n",
      "are           0.0    0.0    1.0    0.0    0.0    0.0    0.0\n",
      "basketball    0.0    0.0    0.0    0.0    0.0    1.0    1.0\n",
      "cat           1.0    1.0    0.0    0.0    0.0    0.0    0.0\n",
      "cooking       0.0    0.0    0.0    1.0    0.0    0.0    0.0\n",
      "courts        0.0    0.0    0.0    0.0    0.0    1.0    0.0\n",
      "dog           1.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
      "dogs          0.0    0.0    1.0    0.0    0.0    0.0    0.0\n",
      "enjoy         0.0    0.0    0.0    1.0    0.0    0.0    0.0\n",
      "food          0.0    0.0    0.0    0.0    1.0    0.0    0.0\n",
      "game          0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "herbs         0.0    0.0    0.0    0.0    1.0    0.0    0.0\n",
      "i             1.0    0.0    0.0    1.0    0.0    0.0    0.0\n",
      "in            0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "is            0.0    1.0    0.0    0.0    0.0    0.0    0.0\n",
      "italian       0.0    0.0    0.0    0.0    1.0    0.0    0.0\n",
      "love          1.0    0.0    0.0    0.0    0.0    0.0    0.0\n",
      "loyal         0.0    0.0    1.0    0.0    0.0    0.0    0.0\n",
      "many          0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "my            1.0    1.0    0.0    0.0    0.0    0.0    0.0\n",
      "on            0.0    0.0    0.0    0.0    0.0    1.0    0.0\n",
      "pasta         0.0    0.0    0.0    1.0    0.0    0.0    0.0\n",
      "pets          0.0    0.0    1.0    0.0    0.0    0.0    0.0\n",
      "pizza         0.0    0.0    0.0    1.0    0.0    0.0    0.0\n",
      "play          0.0    0.0    0.0    0.0    0.0    1.0    0.0\n",
      "player        0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "playful       0.0    1.0    0.0    0.0    0.0    0.0    0.0\n",
      "points        0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "scored        0.0    0.0    0.0    0.0    0.0    0.0    1.0\n",
      "teams         0.0    0.0    0.0    0.0    0.0    1.0    0.0\n",
      "the           0.0    0.0    0.0    0.0    0.0    0.0    2.0\n",
      "tomatoes      0.0    0.0    0.0    0.0    1.0    0.0    0.0\n",
      "uses          0.0    0.0    0.0    0.0    1.0    0.0    0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love my cat and dog.\",\n",
    "    \"My cat is playful.\",\n",
    "    \"Dogs are loyal pets.\",\n",
    "    \"I enjoy cooking pasta and pizza.\",\n",
    "    \"Italian food uses tomatoes and herbs.\",\n",
    "    \"Basketball teams play on courts.\",\n",
    "    \"The player scored many points in the basketball game.\"\n",
    "]\n",
    "\n",
    "# Step 1: Tokenize (split into words and normalize)\n",
    "def simple_tokenize(text):\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    return text.lower().replace('.', '').replace(',', '').split()\n",
    "\n",
    "tokenized_docs = [simple_tokenize(doc) for doc in documents]\n",
    "\n",
    "# Step 2: Create vocabulary (unique words across all documents)\n",
    "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "\n",
    "# Step 3: Create the term-document matrix\n",
    "term_doc_matrix = np.zeros((len(vocab), len(documents)))\n",
    "\n",
    "# Fill the matrix with word counts\n",
    "for doc_idx, doc in enumerate(tokenized_docs):\n",
    "    word_counts = Counter(doc)\n",
    "    for word_idx, word in enumerate(vocab):\n",
    "        term_doc_matrix[word_idx, doc_idx] = word_counts.get(word, 0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df = pd.DataFrame(term_doc_matrix, index=vocab, columns=[f\"doc_{i+1}\" for i in range(len(documents))])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"\\nTerm-Document Matrix:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ‚öôÔ∏è How the code works\n",
    "\n",
    "1. We start with a list of simple documents (sentences)\n",
    "2. We tokenize each document by converting to lowercase, removing punctuation, and splitting into words\n",
    "3. We create a vocabulary of all unique words across documents\n",
    "4. We initialize a matrix of zeros with dimensions (vocab_size √ó num_documents)\n",
    "5. For each document, we:\n",
    "   - Count how many times each word appears\n",
    "   - Update the corresponding matrix cell\n",
    "6. We display the result as a pandas DataFrame for readability\n",
    "\n",
    "### üìñ **Measuring Word Similarity**\n",
    "\n",
    "Once we have our term-document matrix, we can measure word similarity! The most common way is using **cosine similarity**:\n",
    "\n",
    "$$\\text{cosine}(\\vec{v}, \\vec{w}) = \\frac{\\vec{v} \\cdot \\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum_{i=1}^{n} v_i w_i}{\\sqrt{\\sum_{i=1}^{n} v_i^2} \\sqrt{\\sum_{i=1}^{n} w_i^2}}$$\n",
    "\n",
    "Cosine similarity:\n",
    "- Ranges from -1 (opposite) to 1 (identical)\n",
    "- Measures the angle between vectors, not their magnitude\n",
    "- Value of 1 means vectors point in same direction (similar context)\n",
    "- Value of 0 means vectors are orthogonal (unrelated context)\n",
    "\n",
    "### üíª Code section: Calculating Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities with 'cat':\n",
      "  cat - dog: 0.7071\n",
      "  cat - playful: 0.7071\n",
      "  cat - cooking: 0.0000\n",
      "  cat - basketball: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the vectors for specific words\n",
    "def get_word_vector(word, vocab, matrix):\n",
    "    if word in vocab:\n",
    "        word_idx = vocab.index(word)\n",
    "        return matrix[word_idx, :]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Let's calculate similarity between 'cat' and various words\n",
    "target_word = 'cat'\n",
    "comparison_words = ['dog', 'playful', 'cooking', 'basketball']\n",
    "\n",
    "target_vector = get_word_vector(target_word, vocab, term_doc_matrix)\n",
    "\n",
    "if target_vector is not None:\n",
    "    print(f\"Similarities with '{target_word}':\")\n",
    "    for word in comparison_words:\n",
    "        comp_vector = get_word_vector(word, vocab, term_doc_matrix)\n",
    "        if comp_vector is not None:\n",
    "            similarity = cosine_similarity([target_vector], [comp_vector])[0][0]\n",
    "            print(f\"  {target_word} - {word}: {similarity:.4f}\")\n",
    "        else:\n",
    "            print(f\"  '{word}' not in vocabulary\")\n",
    "else:\n",
    "    print(f\"'{target_word}' not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üß™ Try it yourself\n",
    "\n",
    "1. Try changing the `target_word` to different words in our vocabulary.\n",
    "2. Add new comparison words to see how they relate to your target word.\n",
    "3. What words have the highest similarity with 'basketball'? Why do you think that is?\n",
    "\n",
    "### ‚ö° Challenge: Raw Count Limitations\n",
    "\n",
    "Raw counts in our term-document matrix have a major problem: common words like \"the\", \"a\", \"is\" will appear in almost all documents regardless of topic. This gives them unfairly high similarity with all other words!\n",
    "\n",
    "How might we fix this problem?\n",
    "\n",
    "## 4. TF-IDF: Improving Term-Document Matrices\n",
    "\n",
    "### üìñ **TF-IDF: Term Frequency-Inverse Document Frequency**\n",
    "\n",
    "TF-IDF is a clever weighting scheme that addresses the problem of common words by:\n",
    "1. Giving higher weight to words that appear frequently in a document (TF)\n",
    "2. Giving lower weight to words that appear in many documents (IDF)\n",
    "\n",
    "The formula for TF-IDF is:\n",
    "\n",
    "$$\\text{tf-idf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{tf}(t, d)$ is the term frequency of term $t$ in document $d$\n",
    "- $\\text{idf}(t, D)$ is the inverse document frequency of term $t$ in the document set $D$\n",
    "\n",
    "For IDF, we use:\n",
    "\n",
    "$$\\text{idf}(t, D) = \\log\\frac{N}{1 + \\text{df}(t, D)}$$\n",
    "\n",
    "Where:\n",
    "- $N$ is the total number of documents\n",
    "- $\\text{df}(t, D)$ is the number of documents that contain term $t$\n",
    "\n",
    "### üí° Curiosity: Why add 1 to the denominator?\n",
    "\n",
    "The \"+1\" in the denominator is a smoothing term to prevent division by zero if a term doesn't appear in any documents. This version of IDF is sometimes called \"smooth IDF.\"\n",
    "\n",
    "### üíª Code section: Implementing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (words √ó documents):\n",
      "               doc_1     doc_2    doc_3  doc_4     doc_5     doc_6     doc_7\n",
      "basketball  0.000000  0.000000  0.00000    0.0  0.000000  0.432182  0.383337\n",
      "cat         0.506202  0.638709  0.00000    0.0  0.000000  0.000000  0.000000\n",
      "cooking     0.000000  0.000000  0.00000    0.5  0.000000  0.000000  0.000000\n",
      "courts      0.000000  0.000000  0.00000    0.0  0.000000  0.520647  0.000000\n",
      "dog         0.609819  0.000000  0.00000    0.0  0.000000  0.000000  0.000000\n",
      "dogs        0.000000  0.000000  0.57735    0.0  0.000000  0.000000  0.000000\n",
      "enjoy       0.000000  0.000000  0.00000    0.5  0.000000  0.000000  0.000000\n",
      "food        0.000000  0.000000  0.00000    0.0  0.447214  0.000000  0.000000\n",
      "game        0.000000  0.000000  0.00000    0.0  0.000000  0.000000  0.461804\n",
      "herbs       0.000000  0.000000  0.00000    0.0  0.447214  0.000000  0.000000\n",
      "italian     0.000000  0.000000  0.00000    0.0  0.447214  0.000000  0.000000\n",
      "love        0.609819  0.000000  0.00000    0.0  0.000000  0.000000  0.000000\n",
      "loyal       0.000000  0.000000  0.57735    0.0  0.000000  0.000000  0.000000\n",
      "pasta       0.000000  0.000000  0.00000    0.5  0.000000  0.000000  0.000000\n",
      "pets        0.000000  0.000000  0.57735    0.0  0.000000  0.000000  0.000000\n",
      "pizza       0.000000  0.000000  0.00000    0.5  0.000000  0.000000  0.000000\n",
      "play        0.000000  0.000000  0.00000    0.0  0.000000  0.520647  0.000000\n",
      "player      0.000000  0.000000  0.00000    0.0  0.000000  0.000000  0.461804\n",
      "playful     0.000000  0.769449  0.00000    0.0  0.000000  0.000000  0.000000\n",
      "points      0.000000  0.000000  0.00000    0.0  0.000000  0.000000  0.461804\n",
      "scored      0.000000  0.000000  0.00000    0.0  0.000000  0.000000  0.461804\n",
      "teams       0.000000  0.000000  0.00000    0.0  0.000000  0.520647  0.000000\n",
      "tomatoes    0.000000  0.000000  0.00000    0.0  0.447214  0.000000  0.000000\n",
      "uses        0.000000  0.000000  0.00000    0.0  0.447214  0.000000  0.000000\n",
      "\n",
      "TF-IDF values for document 1 (I love my cat and dog.):\n",
      "dog           0.609819\n",
      "love          0.609819\n",
      "cat           0.506202\n",
      "basketball    0.000000\n",
      "pets          0.000000\n",
      "Name: doc_1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Original documents\n",
    "documents = [\n",
    "    \"I love my cat and dog.\",\n",
    "    \"My cat is playful.\",\n",
    "    \"Dogs are loyal pets.\",\n",
    "    \"I enjoy cooking pasta and pizza.\",\n",
    "    \"Italian food uses tomatoes and herbs.\",\n",
    "    \"Basketball teams play on courts.\",\n",
    "    \"The player scored many points in the basketball game.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"doc_{i+1}\" for i in range(len(documents))]\n",
    ").T  # Transpose to match our earlier format\n",
    "\n",
    "print(\"TF-IDF Matrix (words √ó documents):\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Let's examine a specific document\n",
    "doc_idx = 0  # First document\n",
    "print(f\"\\nTF-IDF values for document {doc_idx+1} ({documents[doc_idx]}):\")\n",
    "# Sort values in descending order to see the most important words\n",
    "doc_tfidf = tfidf_df.iloc[:, doc_idx].sort_values(ascending=False)\n",
    "print(doc_tfidf.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚öôÔ∏è How TF-IDF works\n",
    "\n",
    "1. **Term Frequency (TF)**: Counts how many times a term appears in a document, often normalized by document length\n",
    "2. **Inverse Document Frequency (IDF)**: Measures how unique or rare a term is across all documents\n",
    "3. **Multiplication**: When we multiply these values, we get a weight that is:\n",
    "   - High for terms that appear frequently in a few documents\n",
    "   - Low for terms that appear in many documents\n",
    "   - Low for terms that appear rarely in a document\n",
    "\n",
    "TF-IDF helps us identify the most distinctive or important terms in each document.\n",
    "\n",
    "### üß™ Try it yourself: Document Similarity with TF-IDF\n",
    "\n",
    "Now that we have TF-IDF vectors for our documents, let's find similar documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity Matrix:\n",
      "          doc_1     doc_2  doc_3  doc_4  doc_5     doc_6     doc_7\n",
      "doc_1  1.000000  0.323316    0.0    0.0    0.0  0.000000  0.000000\n",
      "doc_2  0.323316  1.000000    0.0    0.0    0.0  0.000000  0.000000\n",
      "doc_3  0.000000  0.000000    1.0    0.0    0.0  0.000000  0.000000\n",
      "doc_4  0.000000  0.000000    0.0    1.0    0.0  0.000000  0.000000\n",
      "doc_5  0.000000  0.000000    0.0    0.0    1.0  0.000000  0.000000\n",
      "doc_6  0.000000  0.000000    0.0    0.0    0.0  1.000000  0.165671\n",
      "doc_7  0.000000  0.000000    0.0    0.0    0.0  0.165671  1.000000\n",
      "\n",
      "Documents most similar to doc_1:\n",
      "doc_1    1.000000\n",
      "doc_2    0.323316\n",
      "doc_3    0.000000\n",
      "doc_4    0.000000\n",
      "doc_5    0.000000\n",
      "doc_6    0.000000\n",
      "doc_7    0.000000\n",
      "Name: doc_1, dtype: float64\n",
      "\n",
      "Document contents:\n",
      "doc_1: I love my cat and dog.\n",
      "doc_2: My cat is playful.\n",
      "doc_3: Dogs are loyal pets.\n",
      "doc_4: I enjoy cooking pasta and pizza.\n",
      "doc_5: Italian food uses tomatoes and herbs.\n",
      "doc_6: Basketball teams play on courts.\n",
      "doc_7: The player scored many points in the basketball game.\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity between documents\n",
    "doc_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(\n",
    "    doc_similarity,\n",
    "    index=[f\"doc_{i+1}\" for i in range(len(documents))],\n",
    "    columns=[f\"doc_{i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"Document Similarity Matrix:\")\n",
    "print(similarity_df)\n",
    "\n",
    "# Let's find the most similar document to document 1\n",
    "doc_idx = 0\n",
    "similarities = similarity_df.iloc[doc_idx].sort_values(ascending=False)\n",
    "print(f\"\\nDocuments most similar to doc_{doc_idx+1}:\")\n",
    "print(similarities)\n",
    "\n",
    "# Print the actual documents for comparison\n",
    "print(\"\\nDocument contents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"doc_{i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word-Context Matrix and PPMI\n",
    "\n",
    "### üìñ **Word-Context Matrix**\n",
    "\n",
    "While term-document matrices are useful, they're limited by the number of documents. A more flexible approach is to use a **word-context matrix**:\n",
    "\n",
    "- Each row represents a target word\n",
    "- Each column represents a context word (words that appear near the target)\n",
    "- Each cell shows how often the context word appears near the target word\n",
    "\n",
    "For example, we might count how often words appear within a 2-word window of our target:\n",
    "\n",
    "|          | cat  | dog  | play | food | ball |\n",
    "|----------|------|------|------|------|------|\n",
    "| pet      | 15   | 18   | 5    | 3    | 2    |\n",
    "| animal   | 10   | 12   | 3    | 1    | 0    |\n",
    "| game     | 0    | 1    | 12   | 0    | 15   |\n",
    "| eat      | 2    | 3    | 0    | 20   | 0    |\n",
    "\n",
    "### üíª Code section: Creating a Word-Context Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Context Matrix (showing a subset):\n",
      "      dogs  cats  pets  play  food  chase  toys\n",
      "dogs   0.0   1.0   1.0   0.0   0.0    0.0   0.0\n",
      "cats   1.0   0.0   0.0   0.0   0.0    1.0   1.0\n",
      "pets   1.0   0.0   0.0   0.0   2.0    0.0   0.0\n",
      "play   0.0   0.0   0.0   0.0   0.0    0.0   1.0\n",
      "food   0.0   0.0   2.0   0.0   2.0    0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Dogs and cats are common pets. People love their pets.\n",
    "Dogs can be trained to play games. Some dogs like to chase balls.\n",
    "Cats like to play with toys. Many cats enjoy chasing mice.\n",
    "People feed their pets special food. Healthy food keeps pets active.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and normalize\n",
    "words = text.lower().replace('.', ' ').replace('\\n', ' ').split()\n",
    "\n",
    "# Create a function to get context words\n",
    "def get_context_words(words, target_idx, window_size=2):\n",
    "    \"\"\"Get context words within the specified window size.\"\"\"\n",
    "    start = max(0, target_idx - window_size)\n",
    "    end = min(len(words), target_idx + window_size + 1)\n",
    "    return [words[i] for i in range(start, end) if i != target_idx]\n",
    "\n",
    "# Count co-occurrences\n",
    "word_context_counts = {}\n",
    "\n",
    "for i, target in enumerate(words):\n",
    "    if target not in word_context_counts:\n",
    "        word_context_counts[target] = Counter()\n",
    "    \n",
    "    context_words = get_context_words(words, i)\n",
    "    word_context_counts[target].update(context_words)\n",
    "\n",
    "# Get unique words for rows and columns\n",
    "target_words = sorted(word_context_counts.keys())\n",
    "context_words = sorted(set(word for counts in word_context_counts.values() for word in counts))\n",
    "\n",
    "# Create the word-context matrix\n",
    "matrix = np.zeros((len(target_words), len(context_words)))\n",
    "\n",
    "for i, target in enumerate(target_words):\n",
    "    for j, context in enumerate(context_words):\n",
    "        matrix[i, j] = word_context_counts[target].get(context, 0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "word_context_df = pd.DataFrame(matrix, index=target_words, columns=context_words)\n",
    "\n",
    "print(\"Word-Context Matrix (showing a subset):\")\n",
    "# Select a subset of interesting words for readability\n",
    "subset_targets = ['dogs', 'cats', 'pets', 'play', 'food']\n",
    "subset_contexts = ['dogs', 'cats', 'pets', 'play', 'food', 'chase', 'toys']\n",
    "\n",
    "# Filter the DataFrame to show only selected words\n",
    "subset_df = word_context_df.loc[\n",
    "    [w for w in subset_targets if w in word_context_df.index],\n",
    "    [w for w in subset_contexts if w in word_context_df.columns]\n",
    "]\n",
    "print(subset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üìñ **PPMI: Positive Pointwise Mutual Information**\n",
    "\n",
    "Raw co-occurrence counts have similar problems to our term-document matrix. Common words will co-occur frequently with everything!\n",
    "\n",
    "**Pointwise Mutual Information (PMI)** measures the association between words by comparing how often they occur together versus how often we'd expect them to occur together by chance:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n",
    "\n",
    "Where:\n",
    "- $P(w, c)$ is the probability of seeing word $w$ and context $c$ together\n",
    "- $P(w)$ is the probability of word $w$\n",
    "- $P(c)$ is the probability of context $c$\n",
    "\n",
    "**Positive PMI (PPMI)** sets all negative PMI values to zero:\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n",
    "\n",
    "### üí° Curiosity: Interpreting PMI\n",
    "\n",
    "- PMI = 0: Words occur together exactly as often as expected by chance\n",
    "- PMI > 0: Words occur together more often than expected by chance\n",
    "- PMI < 0: Words occur together less often than expected by chance\n",
    "\n",
    "We use PPMI because negative PMI values are often unreliable unless we have enormous corpora.\n",
    "\n",
    "### üíª Code section: Calculating PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PPMI Matrix (showing the same subset):\n",
      "      dogs  cats  pets  play  food  chase  toys\n",
      "dogs  0.00  0.54  0.21   0.0  0.00   0.00  0.00\n",
      "cats  0.54  0.00  0.00   0.0  0.00   1.86  1.86\n",
      "pets  0.21  0.00  0.00   0.0  1.54   0.00  0.00\n",
      "play  0.00  0.00  0.00   0.0  0.00   0.00  2.44\n",
      "food  0.00  0.00  1.54   0.0  2.44   0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate PPMI from our word-context matrix\n",
    "def calculate_ppmi(matrix):\n",
    "    # Convert counts to probabilities\n",
    "    total_count = np.sum(matrix)\n",
    "    \n",
    "    # Probability of (word, context) pair\n",
    "    p_w_c = matrix / total_count\n",
    "    \n",
    "    # Marginal probabilities\n",
    "    p_w = np.sum(matrix, axis=1) / total_count  # Sum over contexts\n",
    "    p_c = np.sum(matrix, axis=0) / total_count  # Sum over words\n",
    "    \n",
    "    # Calculate PMI\n",
    "    # Add a small constant to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    pmi = np.zeros(matrix.shape)\n",
    "    \n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] > 0:  # Only calculate for non-zero counts\n",
    "                pmi[i, j] = np.log2((p_w_c[i, j] + epsilon) / ((p_w[i] + epsilon) * (p_c[j] + epsilon)))\n",
    "    \n",
    "    # Convert to PPMI (replace negative values with 0)\n",
    "    ppmi = np.maximum(pmi, 0)\n",
    "    \n",
    "    return ppmi\n",
    "\n",
    "# Calculate PPMI\n",
    "ppmi_matrix = calculate_ppmi(matrix)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "ppmi_df = pd.DataFrame(ppmi_matrix, index=target_words, columns=context_words)\n",
    "\n",
    "print(\"\\nPPMI Matrix (showing the same subset):\")\n",
    "# Filter the DataFrame to show only selected words\n",
    "subset_ppmi_df = ppmi_df.loc[\n",
    "    [w for w in subset_targets if w in ppmi_df.index],\n",
    "    [w for w in subset_contexts if w in ppmi_df.columns]\n",
    "]\n",
    "print(subset_ppmi_df.round(2))  # Round to 2 decimal places for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ‚öôÔ∏è How PPMI works\n",
    "\n",
    "1. PPMI measures how much more frequently two words occur together than we would expect by random chance\n",
    "2. High PPMI values indicate strong association between words\n",
    "3. Zero PPMI values indicate either:\n",
    "   - The words occur together about as often as expected by chance, or\n",
    "   - The words occur together less often than expected by chance\n",
    "\n",
    "### üîé Deepening the concept: Word similarity with PPMI\n",
    "\n",
    "The PPMI matrix gives us context vectors for each word that highlight their most significant associations. Let's examine the similarities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Similarity Matrix (based on PPMI vectors):\n",
      "       dogs  cats  pets  play  food  chase\n",
      "dogs   1.00  0.22  0.17  0.42  0.01   0.18\n",
      "cats   0.22  1.00  0.18  0.18  0.00   0.31\n",
      "pets   0.17  0.18  1.00  0.00  0.35   0.00\n",
      "play   0.42  0.18  0.00  1.00  0.00   0.24\n",
      "food   0.01  0.00  0.35  0.00  1.00   0.00\n",
      "chase  0.18  0.31  0.00  0.24  0.00   1.00\n",
      "\n",
      "Words most similar to 'cats':\n",
      "cats     1.000000\n",
      "are      0.456693\n",
      "with     0.397019\n",
      "enjoy    0.351894\n",
      "many     0.351894\n",
      "Name: cats, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity between target words using PPMI vectors\n",
    "word_similarities = cosine_similarity(ppmi_matrix)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "word_sim_df = pd.DataFrame(word_similarities, index=target_words, columns=target_words)\n",
    "\n",
    "print(\"\\nWord Similarity Matrix (based on PPMI vectors):\")\n",
    "# Select a subset of words\n",
    "subset_words = ['dogs', 'cats', 'pets', 'play', 'food', 'chase']\n",
    "subset_sim_df = word_sim_df.loc[\n",
    "    [w for w in subset_words if w in word_sim_df.index],\n",
    "    [w for w in subset_words if w in word_sim_df.columns]\n",
    "]\n",
    "print(subset_sim_df.round(2))\n",
    "\n",
    "# Find most similar words for 'cats'\n",
    "if 'cats' in target_words:\n",
    "    cat_idx = target_words.index('cats')\n",
    "    cat_similarities = word_sim_df.iloc[cat_idx].sort_values(ascending=False)\n",
    "    print(\"\\nWords most similar to 'cats':\")\n",
    "    print(cat_similarities.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. From Sparse to Dense: Word2Vec\n",
    "\n",
    "### üìñ **The Limits of Sparse Vectors**\n",
    "\n",
    "While PPMI and TF-IDF are powerful, they have limitations:\n",
    "- Very high dimensionality (vocabulary size)\n",
    "- Sparse (mostly zeros)\n",
    "- Need huge amounts of data to get good estimates for rare words\n",
    "\n",
    "**Dense vectors** (embeddings) address these issues:\n",
    "- Lower dimensionality (typically 50-1000)\n",
    "- Dense (mostly non-zero values)\n",
    "- Better at capturing synonymy and other relationships\n",
    "\n",
    "### üìñ **Word2Vec: Learning Dense Embeddings**\n",
    "\n",
    "Word2Vec is a family of models that learn word embeddings by training a shallow neural network on a simple prediction task.\n",
    "\n",
    "The skip-gram model works by:\n",
    "1. Taking a word and predicting context words that appear near it\n",
    "2. Learning word vectors that are good at this prediction task\n",
    "3. Using these vectors as our word embeddings\n",
    "\n",
    "The task is: \"Given a target word, is word c likely to appear nearby?\"\n",
    "\n",
    "### üí° Curiosity: Self-Supervision\n",
    "\n",
    "A revolutionary insight in Word2Vec is the use of \"self-supervision\" - creating a supervised task from unlabeled text. The model uses each word's context as its own supervision signal!\n",
    "\n",
    "### üìñ **Skip-gram with Negative Sampling (SGNS)**\n",
    "\n",
    "The most common Word2Vec algorithm is skip-gram with negative sampling:\n",
    "1. For each target word, we have positive examples (actual context words)\n",
    "2. We also create negative examples by randomly sampling words from the vocabulary\n",
    "3. We train a binary classifier to distinguish real context words from random words\n",
    "4. The weights learned by this classifier become our word embeddings\n",
    "\n",
    "### üíª Code section: Using Word2Vec with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 59\n",
      "Some words in vocabulary: ['dogs', 'cats', 'pets', 'to', 'many', 'people', 'their', 'animals', 'like', 'pet']\n",
      "\n",
      "Vector for 'dogs':\n",
      "[-0.00053835  0.00024034  0.00510206  0.00900546 -0.00931295 -0.00713388\n",
      "  0.00644987  0.00897796 -0.00503696 -0.00376834]\n",
      "\n",
      "Words most similar to 'dogs':\n",
      "  with: 0.2190\n",
      "  pet: 0.2162\n",
      "  and: 0.1954\n",
      "  sleep: 0.1723\n",
      "  help: 0.1694\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Our tokenized sentences (we need more data for good embeddings)\n",
    "sentences = [\n",
    "    \"dogs and cats are common pets\".split(),\n",
    "    \"people love their pets\".split(),\n",
    "    \"dogs can be trained to play games\".split(),\n",
    "    \"some dogs like to chase balls\".split(),\n",
    "    \"cats like to play with toys\".split(),\n",
    "    \"many cats enjoy chasing mice\".split(),\n",
    "    \"people feed their pets special food\".split(),\n",
    "    \"healthy food keeps pets active\".split(),\n",
    "    \"dogs need regular exercise\".split(),\n",
    "    \"cats sleep for many hours each day\".split(),\n",
    "    \"pet owners take care of their animals\".split(),\n",
    "    \"veterinarians help sick pets get better\".split(),\n",
    "    \"dogs can learn many different tricks\".split(),\n",
    "    \"cats are often independent animals\".split(),\n",
    "    \"some people prefer dogs as pets\".split(),\n",
    "    \"others prefer to have pet cats\".split()\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "# Parameters:\n",
    "# - sentences: our input text\n",
    "# - vector_size: the dimensionality of our word vectors\n",
    "# - window: how many words to consider as context\n",
    "# - min_count: ignore words that appear less than this\n",
    "# - workers: threads for training\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = list(model.wv.index_to_key)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Some words in vocabulary: {vocab[:10]}\")\n",
    "\n",
    "# Get vector for a specific word\n",
    "print(\"\\nVector for 'dogs':\")\n",
    "print(model.wv['dogs'][:10])  # Show only first 10 dimensions for readability\n",
    "\n",
    "# Find most similar words\n",
    "print(\"\\nWords most similar to 'dogs':\")\n",
    "similar_words = model.wv.most_similar('dogs', topn=5)\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"  {word}: {similarity:.4f}\")\n",
    "\n",
    "# Visualize some word vectors in 2D space\n",
    "def plot_words(model, words):\n",
    "    # Get the word vectors\n",
    "    word_vectors = [model.wv[word] for word in words]\n",
    "    \n",
    "    # Reduce to 2 dimensions with PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(result[:, 0], result[:, 1], c='blue', alpha=0.7)\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "    \n",
    "    plt.title(\"Word Embeddings visualized in 2D space\", fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Select some interesting words to visualize\n",
    "words_to_plot = ['dogs', 'cats', 'pets', 'play', 'chase', 'food', 'people', 'animals', 'toys', 'mice']\n",
    "words_to_plot = [w for w in words_to_plot if w in vocab]\n",
    "plot_words(model, words_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Challenge: Small Dataset Limitations\n",
    "\n",
    "You'll notice that our Word2Vec model isn't very good with this tiny dataset. In real applications, Word2Vec is typically trained on billions of words! What kinds of problems do you see with our small model?\n",
    "\n",
    "### üìñ **How Skip-gram Actually Works**\n",
    "\n",
    "Let's break down the mathematics behind skip-gram:\n",
    "\n",
    "1. We have two embeddings for each word:\n",
    "   - Target embeddings (when word is the center word)\n",
    "   - Context embeddings (when word is a context word)\n",
    "\n",
    "2. For a target word $w$ and context word $c$, the probability of $c$ being a real context word is:\n",
    "   \n",
    "   $$P(+|w, c) = \\sigma(c \\cdot w) = \\frac{1}{1 + e^{-c \\cdot w}}$$\n",
    "\n",
    "   Where $\\sigma$ is the sigmoid function and $c \\cdot w$ is the dot product.\n",
    "\n",
    "3. The training objective maximizes the probability of real context words and minimizes the probability of random \"negative sample\" words.\n",
    "\n",
    "### üí° Curiosity: Word Analogies\n",
    "\n",
    "One of the most fascinating properties of Word2Vec embeddings is their ability to capture analogies through vector arithmetic. The classic example is:\n",
    "\n",
    "$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n",
    "\n",
    "This means that the vector difference between \"king\" and \"man\" captures something like a \"royalty\" relationship, which when added to \"woman\" gives us \"queen\".\n",
    "\n",
    "### üß™ Try it yourself: Using Pre-trained Embeddings\n",
    "\n",
    "In practice, you'll often use pre-trained embeddings rather than training your own. Let's see how to use pre-trained GloVe embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Function to download and load GloVe embeddings\n",
    "def load_glove_embeddings(dimension=50):\n",
    "    \"\"\"\n",
    "    Downloads and loads GloVe embeddings with the specified dimension.\n",
    "    Returns a dictionary mapping words to their vectors.\n",
    "    \"\"\"\n",
    "    # Check if we've already downloaded the file\n",
    "    glove_file = f\"glove.6B.{dimension}d.txt\"\n",
    "    \n",
    "    if not os.path.exists(glove_file):\n",
    "        # URL for the GloVe 6B embeddings\n",
    "        url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "        \n",
    "        print(\"Downloading GloVe embeddings...\")\n",
    "        r = requests.get(url)\n",
    "        \n",
    "        # Unzip the file\n",
    "        print(\"Extracting zip file...\")\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall()\n",
    "        \n",
    "        print(f\"Downloaded and extracted GloVe embeddings to {os.getcwd()}\")\n",
    "    \n",
    "    # Load the embeddings\n",
    "    print(f\"Loading {dimension}-dimensional GloVe embeddings...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array([float(val) for val in values[1:]])\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings)} word vectors.\")\n",
    "    return embeddings\n",
    "\n",
    "# Load the embeddings (small dimension for faster loading)\n",
    "# Note: This will download a ~800MB file if you run it!\n",
    "# For the notebook, we'll comment this out to avoid unintended downloads\n",
    "# glove_embeddings = load_glove_embeddings(50)\n",
    "\n",
    "# Example: Computing analogies (king - man + woman = ?)\n",
    "def compute_analogy(embeddings, word1, word2, word3):\n",
    "    \"\"\"\n",
    "    Computes word analogy: word1 - word2 + word3 = ?\n",
    "    Returns the most similar word to the resulting vector.\n",
    "    \"\"\"\n",
    "    # Check if all words are in the vocabulary\n",
    "    for word in [word1, word2, word3]:\n",
    "        if word not in embeddings:\n",
    "            print(f\"'{word}' not in vocabulary.\")\n",
    "            return None\n",
    "    \n",
    "    # Compute the analogy vector\n",
    "    target_vector = embeddings[word1] - embeddings[word2] + embeddings[word3]\n",
    "    \n",
    "    # Find the most similar word\n",
    "    best_word = None\n",
    "    best_similarity = -float('inf')\n",
    "    \n",
    "    for word, vector in embeddings.items():\n",
    "        # Skip the input words\n",
    "        if word in [word1, word2, word3]:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = np.dot(vector, target_vector) / (np.linalg.norm(vector) * np.linalg.norm(target_vector))\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word, best_similarity\n",
    "\n",
    "# Example usage (commented out to avoid downloading)\n",
    "# result, similarity = compute_analogy(glove_embeddings, 'king', 'man', 'woman')\n",
    "# print(f\"king - man + woman = {result} (similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. Applications and Limitations of Vector Semantics\n",
    "\n",
    "### üìñ **Applications of Vector Semantics**\n",
    "\n",
    "Word embeddings are used in a wide variety of NLP tasks:\n",
    "\n",
    "1. **Finding similar words** (thesaurus creation)\n",
    "2. **Document similarity** (for information retrieval, plagiarism detection)\n",
    "3. **Recommendation systems** (suggesting similar content)\n",
    "4. **Machine translation** (mapping words between languages)\n",
    "5. **Sentiment analysis** (understanding positive/negative opinions)\n",
    "6. **Named entity recognition** (identifying proper nouns)\n",
    "\n",
    "### üìñ **Bias in Embeddings**\n",
    "\n",
    "One major issue with word embeddings is that they capture and amplify biases present in the training data.\n",
    "\n",
    "Examples of problematic analogies found in embeddings:\n",
    "- man : computer_programmer :: woman : homemaker\n",
    "- father : doctor :: mother : nurse\n",
    "\n",
    "These biases aren't just theoretical problems - they can lead to:\n",
    "1. **Allocational harms**: Unfair allocation of resources (e.g., job search algorithms favoring men for certain positions)\n",
    "2. **Representational harms**: Demeaning or ignoring certain social groups\n",
    "\n",
    "### üí° Curiosity: Debiasing Embeddings\n",
    "\n",
    "Researchers have developed techniques to reduce bias in embeddings, such as:\n",
    "1. Identifying and removing the \"gender direction\" in the vector space\n",
    "2. Creating new training objectives that specifically reduce bias\n",
    "\n",
    "However, completely eliminating bias remains an open problem!\n",
    "\n",
    "### üìñ **Contextual vs. Static Embeddings**\n",
    "\n",
    "Static embeddings like Word2Vec and GloVe have one major limitation: a word has the same embedding regardless of context.\n",
    "\n",
    "For example, the word \"bank\" might refer to:\n",
    "- A financial institution\n",
    "- The side of a river\n",
    "- To tilt an aircraft\n",
    "\n",
    "Modern NLP systems use **contextual embeddings** (like BERT, GPT, etc.) that produce different representations for words based on their surrounding context.\n",
    "\n",
    "## 8. Review and Practical Tips\n",
    "\n",
    "### üéØ Summary of Key Concepts\n",
    "\n",
    "1. **Vector semantics** represents words as points in a multidimensional space\n",
    "2. **Sparse representations** (TF-IDF, PPMI) directly count co-occurrences\n",
    "3. **Dense representations** (Word2Vec, GloVe) learn lower-dimensional embeddings that capture semantic relationships\n",
    "4. **Similarity measures** like cosine similarity quantify how related words are\n",
    "5. **Applications** include document similarity, word similarity, and many downstream NLP tasks\n",
    "6. **Limitations** include bias and the inability of static embeddings to handle context\n",
    "\n",
    "### ‚öôÔ∏è Practical Tips for Working with Embeddings\n",
    "\n",
    "1. **Don't reinvent the wheel**: Use pre-trained embeddings for most tasks\n",
    "2. **Choose the right embeddings**: Different embeddings are optimized for different tasks\n",
    "3. **Fine-tune when needed**: You can further train pre-trained embeddings on your specific data\n",
    "4. **Be aware of bias**: Consider debiasing techniques for sensitive applications\n",
    "5. **Consider contextual embeddings**: For state-of-the-art performance, use contextual models like BERT\n",
    "\n",
    "### üß™ Final quiz: Test your understanding\n",
    "\n",
    "1. What's the main difference between sparse and dense word vectors?\n",
    "2. Why is TF-IDF better than raw word counts for document similarity?\n",
    "3. What problem does PPMI solve compared to raw co-occurrence counts?\n",
    "4. How does Word2Vec learn word embeddings without explicit supervision?\n",
    "5. Name one way that bias can manifest in word embeddings.\n",
    "\n",
    "<details>\n",
    "<summary>Click to see answers</summary>\n",
    "\n",
    "1. Sparse vectors have mostly zeros and dimensions equal to vocabulary size, while dense vectors have mostly non-zero values and lower dimensionality (50-1000).\n",
    "2. TF-IDF downweights common words that appear in many documents, focusing instead on distinctive words that characterize a document.\n",
    "3. PPMI measures how much more frequently words co-occur than expected by chance, reducing the impact of common words that co-occur with everything.\n",
    "4. Word2Vec uses self-supervision - it creates a prediction task (predicting context words given a target word) and uses the text itself as training data.\n",
    "5. Examples include gender stereotypes in occupation words (e.g., programmer associated with male terms), racial biases in sentiment associations, or cultural stereotypes.\n",
    "</details>\n",
    "\n",
    "## 9. Further Learning Resources\n",
    "\n",
    "If you want to learn more about vector semantics and word embeddings, here are some excellent resources:\n",
    "\n",
    "1. **Books**:\n",
    "   - \"Speech and Language Processing\" by Jurafsky & Martin (Chapter 6)\n",
    "   - \"Natural Language Processing with PyTorch\" by Rao & McMahan\n",
    "\n",
    "2. **Online Courses**:\n",
    "   - Stanford CS224N: Natural Language Processing with Deep Learning\n",
    "   - Fast.ai NLP Course\n",
    "\n",
    "3. **Papers**:\n",
    "   - \"Efficient Estimation of Word Representations in Vector Space\" (original Word2Vec paper)\n",
    "   - \"GloVe: Global Vectors for Word Representation\"\n",
    "   - \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\"\n",
    "\n",
    "4. **Tools and Libraries**:\n",
    "   - Gensim: Full-featured library for vector semantics\n",
    "   - spaCy: NLP library with pre-trained embeddings\n",
    "   - TensorFlow Text & TensorFlow Hub: Embeddings and text processing tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-journey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
