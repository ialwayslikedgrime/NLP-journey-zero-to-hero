{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to add:\n",
    "\n",
    "different section:\n",
    "- rnn\n",
    "- lstm\n",
    "- attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE TRANSFORMER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook we introduce the Transformer, which is the architecture that has revolutionized NLP.\n",
    "\n",
    "it was introduced in the paper \"Attention is All you Need\" (2017), which I suggest as primary source of information [https://arxiv.org/abs/1706.03762]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main motivations for introducing the Transformer was to overcome the sequential nature of Recurrent Neural Networks and LSTMs, which process input tokens one at a time sequentially. This sequential dependency makes training slow and limits parallelization. In contrast, Transformers allow for parallel computation across sequence positions, making them significantly more efficient, especially when leveraging GPUs. In fact GPUs are designed to perform many operations in parallel, which is exactly what Transformers exploit, unlike RNNs, which are inherently sequential and cannot fully utilize GPU parallelism.\n",
    "\n",
    "In the case of RNNs and LSTMs, therefore, the sequence is examined just one tokent per time, sequentially. On the other hand, the Transformer relies entirely on the **attention mechanism**, looking at all tokens at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Transformere is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary : \n",
    "\n",
    "- RNNs process inputs token by token: at time step t, you need the hidden state from time step t-1. \n",
    "- This means: each step depends on the previous one, so you can’t compute them in parallel — even with a GPU.\n",
    "- GPUs sit mostly idle, waiting for sequential computations to finish.\n",
    "\n",
    "Transformers: Parallelism Friendly\n",
    "- Transformers compute all token representations at once (self-attention allows looking at all tokens simultaneously).\n",
    "- No time-step dependencies like in RNNs.\n",
    "- Enables parallel matrix multiplications over entire sequences — which GPUs handle extremely efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers model may use a lot of energy consumption. training a model may use the energy consumption of running a household for several years ! Moreover, they may not even fit the RAM of a computer. \n",
    "A possible solution that has been developed is using **distillation**. How does distillation work ? You traing a distilled model (such as **DistillBERT**) by using as label data the predictions of the \"parent\" model - in this case, BERT. surprisingly, in this way, the resulting model performs better than if it was directly trained on the same training data the parent model was trained on. And it is way more portable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
