{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to add:\n",
    "\n",
    "different section:\n",
    "- rnn\n",
    "- lstm\n",
    "- attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE TRANSFORMER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook we introduce the Transformer, which is the architecture that has revolutionized NLP. Today, it is still the standard architecture for building large language models. \n",
    "\n",
    "it was introduced in the paper \"Attention is All you Need\" (2017), which I suggest as primary source of information [Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESOURCES**:\n",
    "\n",
    "### **BOOKS**\n",
    "[Hands-On Large Language Models](https://www.llm-book.com/)\n",
    "\n",
    "### **ONLINE RESOURCES**\n",
    "(from chapter 3 of the \"Hands-On Large Language Models\" book):\n",
    "[Transformer illustrated explanation](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "\n",
    "### **PAPERS**\n",
    "\n",
    "[Attention (2014)](https://arxiv.org/abs/1409.0473). \n",
    "\n",
    "[Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "[BERT (2018)](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main motivations for introducing the Transformer was to overcome the sequential nature of Recurrent Neural Networks and LSTMs, which process input tokens one at a time sequentially. This sequential dependency makes training slow and limits parallelization. In contrast, Transformers allow for parallel computation across sequence positions, making them significantly more efficient, especially when leveraging GPUs. In fact GPUs are designed to perform many operations in parallel, which is exactly what Transformers exploit, unlike RNNs, which are inherently sequential and cannot fully utilize GPU parallelism.\n",
    "\n",
    "In the case of RNNs and LSTMs, therefore, the sequence is examined just one tokent per time, sequentially. On the other hand, the Transformer relies entirely on the **attention mechanism**, looking at all tokens at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Transformere is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRANSFORMER ARCHITECTURE** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention. Attention can be thought as a way to build contextual representions of tokens' meaning by **attending to** and integrating information from sorrounding tokens, helping the model learn how tokens relate to each other over large spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/transformer_architecture.png)\n",
    "\n",
    "\n",
    "*Figure 1: The architecture of a (left-to-right) transformer, showing how each input token get encoded, passed through a set of stacked transformer blocks, and then a language model head that predicts the next token*\n",
    "\n",
    "*NOTE*: \"left-to-right\": \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LEFT-TO-RIGHT OR AUTOREGRESSIVE MODEL**\n",
    "\n",
    "We‚Äôll focus for now on left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer architecture is autoregressive, meaning that it needs to consume each generated word before creating a new word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRANSFORMER BLOCKS**\n",
    "\n",
    "in the picture, the transformer architecture is sketched. We can see the transformer blocks (the purple ones) that are multilayer networks. \n",
    "The transformer blocks are defined as \"multilayer networks\" because they contain various layers stacked up on each other, working together. \n",
    "\n",
    "The Layers inserted are:\n",
    "1) **MULTI-HEAD SELF-ATTENTION LAYER**, which is the main innovation of the transformer architecture. This allows the transformer to focus on different parts of the sentence simultaneously.\n",
    "2) **FEEDFORWARD NETWORK (multilayer perceptrons)** - Usually 2-3 dense layers with nonlinear activations\n",
    "3) **LAYER NORMALIZATION** steps, which are usually applied before or after the other components. \n",
    "4) **SKIP CONNECTIONS** \n",
    "\n",
    "\n",
    "now, the order between skip connections and the layer normalization is not fixed. while in the original paper from 2017 (Attention is All you Need), the skip connections is first, while the normalization layer is later. this is called \"POST-NL\" (post normalization layer)\n",
    "\n",
    "In  more recent architectures, such as BERT or GPT-2, we do have first the normalization layer, and the skip connections after. (PRE-LN variant)\n",
    "\n",
    "That is:\n",
    "\t1.\tLayerNorm first (before attention/FFN).\n",
    "\t2.\tThen sublayer computation.\n",
    "\t3.\tThen add the skip connection.\n",
    "\n",
    "This makes optimization more stable, especially for very deep stacks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**TIP**\n",
    "\n",
    "If any of these four steps are unclear (or **unbekannt** to you), I suggest revisiting the basics of **Machine Learning** or **Deep Learning**.  \n",
    "\n",
    "**üìö RECOMMENDED RESOURCES**\n",
    "\n",
    "- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aur√©lien Geron  \n",
    "  (really great book. easy and practical)  \n",
    "\n",
    "- *Deep Learning* by Ian Goodfellow  \n",
    "  \"Deep Learning\" by Ian Goodfellow (really great book but a bit more mathematics-heavy. Great for understanding deeply the maths. Highly suggest for the part on MLPs and how they work)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the transformer blocks are preceeded by the encodng layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt](images/encoder_decoder_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an encoder-decoder sequence is not new to the Transformer architecture. What's new is using only the attention mechanism. Although RNNs were used also before, they are not great: using RNNs is not the best. Long sequences kept in memory lead to a vanishing gradient. A solution was created by the discovery of the Attention mechanism by (Bahdanau, Cho, Bengio) [Attention Paper (2014)](https://arxiv.org/abs/1409.0473). Even though the full the true power of attention , and what drives the amazing abilities of large language models, was first explored in the well-know [Attention is all you need paper (2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Attention allows a model to focus \"attend\" just to the the most relevant parts. Attention selectively determines which words are most important in a given sentence. For instance, the output word ‚Äúlama‚Äôs‚Äù is Dutch for ‚Äúllamas,‚Äù which is why the attention between both is high. Similarly, the words ‚Äúlama‚Äôs‚Äù and ‚ÄúI‚Äù have lower attention since they aren‚Äôt as related. \n",
    "![Alt](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding these attention mechanisms to the decoder step, the RNN can\n",
    "generate signals for each input word in the sequence related to the potential\n",
    "output. Instead of passing only a context embedding to the decoder, the\n",
    "hidden states of all input words are passed. This process is demonstrated in the\n",
    "Figure \n",
    "\n",
    "![Alt](images/attention_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoding layer\n",
    "\n",
    "takes words as input, embeds them trough an embedding matrix. As we saw in the foundations, we cannot feed a machine just with words. We need numbers. Numbers will have to define not only the specific words, but also grammatical relationships. \n",
    "This is the job of the embedding matrix. It will embed both specific words and also the position of such words (positional embeddings)\n",
    "\n",
    "after the transformer blocks, we do have an unembedding matrix. \n",
    "task of the unembedding matrix is to unembed the vector. As we saw, we need numbers (vectors) to compute operations and actually be able to predict the next word. but actually, we clearly need the final word at the end. Therefore, we need to \"unembed\" the vector, and , through a softmax, choose the most probable word (the predicted word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoder-Decoder; Encoder only; Decoder-only.\n",
    "in the case of the tranformers, the encoder and decoder components are stacked on top of each other. \n",
    "this differs from other architectures, such as **BERT** that is encoder-only, or **GPT**, which is decoder-only. \n",
    "In fact, the vanilla version of BERT, which is encoder-only, can only create a contextual embedding. On the other hand, GPT, which is decoder-only, can only generate output and words.\n",
    "\n",
    "the original Transformers in an encoder-decoder architecture. This works well for some tasks, such as Machine Translation, but cannot be used for different tasks, such as text classification.\n",
    "\n",
    "For these kind of tasks, different architecture were created.\n",
    "An example is the [**bidirectional encoder representations from transformers (2018: BERT)**](https://arxiv.org/abs/1810.04805).\n",
    "Bert is an encoder-only structure. This means that his task is only creating representations of the word embeddings. It focuses only on representing language. It only uses the encoder while removes the decoder entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER BLOCKS ARE STACKED \n",
    "\n",
    "transformer blocks are stacked one over the other.\n",
    "\n",
    "**Why this??**\n",
    "the first blocks learn representation of words. following blocks, instead, learn more complex representations, such as the grammatical rules and semantic structures of the sentence. A column might contain 12 to 96 or more stacked blocks.\n",
    "\n",
    "\n",
    "**Words are represented as vectors of a fixed size**\n",
    "\n",
    "We have seen that words are represented as vectors. The dimension of these vectors is defined initially and stays consistent through all the transformer blocks. Usually, the choice is a vector of 512, 768, 1024 or more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary : \n",
    "\n",
    "- RNNs process inputs token by token: at time step t, you need the hidden state from time step t-1. \n",
    "- This means: each step depends on the previous one, so you can‚Äôt compute them in parallel ‚Äî even with a GPU.\n",
    "- GPUs sit mostly idle, waiting for sequential computations to finish.\n",
    "\n",
    "Transformers: Parallelism Friendly\n",
    "- Transformers compute all token representations at once (self-attention allows looking at all tokens simultaneously).\n",
    "- No time-step dependencies like in RNNs.\n",
    "- Enables parallel matrix multiplications over entire sequences ‚Äî which GPUs handle extremely efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers model may use a lot of energy consumption. training a model may use the energy consumption of running a household for several years ! Moreover, they may not even fit the RAM of a computer. \n",
    "A possible solution that has been developed is using **distillation**. How does distillation work ? You traing a distilled model (such as **DistillBERT**) by using as label data the predictions of the \"parent\" model - in this case, BERT. surprisingly, in this way, the resulting model performs better than if it was directly trained on the same training data the parent model was trained on. And it is way more portable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
