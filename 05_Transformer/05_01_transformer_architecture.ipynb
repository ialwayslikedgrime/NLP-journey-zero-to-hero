{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "want to add:\n",
    "\n",
    "different section:\n",
    "- rnn\n",
    "- lstm\n",
    "- attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE TRANSFORMER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this notebook we introduce the Transformer, which is the architecture that has revolutionized NLP. Today, it is still the standard architecture for building large language models. \n",
    "\n",
    "it was introduced in the paper \"Attention is All you Need\" (2017), which I suggest as primary source of information [Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RESOURCES**:\n",
    "\n",
    "### **BOOKS**\n",
    "[Hands-On Large Language Models](https://www.llm-book.com/)\n",
    "\n",
    "### **ONLINE RESOURCES**\n",
    "(from chapter 3 of the \"Hands-On Large Language Models\" book):\n",
    "[Transformer illustrated explanation](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "\n",
    "### **PAPERS**\n",
    "\n",
    "[Attention (2014)](https://arxiv.org/abs/1409.0473). \n",
    "\n",
    "[Attention is all you need (2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "[BERT (2018)](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "[Flash attention (2022)](https://arxiv.org/abs/2205.14135)\n",
    "\n",
    "\n",
    "### **PREREQUISITES RESOURCES**\n",
    "\n",
    "[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, by AurÃ©lien GÃ©ron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "\n",
    "[Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville](https://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main motivations for introducing the Transformer was to overcome the sequential nature of Recurrent Neural Networks and LSTMs, which process input tokens one at a time sequentially. This sequential dependency makes training slow and limits parallelization. In contrast, Transformers allow for parallel computation across sequence positions, making them significantly more efficient, especially when leveraging GPUs. In fact GPUs are designed to perform many operations in parallel, which is exactly what Transformers exploit, unlike RNNs, which are inherently sequential and cannot fully utilize GPU parallelism.\n",
    "\n",
    "In the case of RNNs and LSTMs, therefore, the sequence is examined just one tokent per time, sequentially. On the other hand, the Transformer relies entirely on the **attention mechanism**, looking at all tokens at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Transformere is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRANSFORMER ARCHITECTURE** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention. Attention can be thought as a way to build contextual representions of tokens' meaning by **attending to** and integrating information from sorrounding tokens, helping the model learn how tokens relate to each other over large spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](images/transformer_architecture.png)\n",
    "\n",
    "\n",
    "*Figure 1: The architecture of a (left-to-right) transformer, showing how each input token get encoded, passed through a set of stacked transformer blocks, and then a language model head that predicts the next token*\n",
    "\n",
    "*NOTE*: \"left-to-right\": \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LEFT-TO-RIGHT OR AUTOREGRESSIVE MODEL**\n",
    "\n",
    "Weâ€™ll focus for now on left-to-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context. The transformer architecture is autoregressive, meaning that it needs to consume each generated word before creating a new word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRANSFORMER BLOCKS**\n",
    "\n",
    "in the picture, the transformer architecture is sketched. We can see the transformer blocks (the purple ones) that are multilayer networks. \n",
    "The transformer blocks are defined as \"multilayer networks\" because they contain various layers stacked up on each other, working together. \n",
    "\n",
    "The Layers inserted are:\n",
    "1) **MULTI-HEAD SELF-ATTENTION LAYER**, which is the main innovation of the transformer architecture. This allows the transformer to focus on different parts of the sentence simultaneously.\n",
    "2) **FEEDFORWARD NETWORK (multilayer perceptrons)** - Usually 2-3 dense layers with nonlinear activations\n",
    "3) **LAYER NORMALIZATION** steps, which are usually applied before or after the other components. \n",
    "4) **SKIP CONNECTIONS** \n",
    "\n",
    "\n",
    "now, the order between skip connections and the layer normalization is not fixed. while in the original paper from 2017 (Attention is All you Need), the skip connections is first, while the normalization layer is later. this is called \"POST-NL\" (post normalization layer)\n",
    "\n",
    "In  more recent architectures, such as BERT or GPT-2, we do have first the normalization layer, and the skip connections after. (PRE-LN variant)\n",
    "\n",
    "That is:\n",
    "\t1.\tLayerNorm first (before attention/FFN).\n",
    "\t2.\tThen sublayer computation.\n",
    "\t3.\tThen add the skip connection.\n",
    "\n",
    "This makes optimization more stable, especially for very deep stacks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**TIP**\n",
    "\n",
    "If any of these four steps are unclear (or **unbekannt** to you), I suggest revisiting the basics of **Machine Learning** or **Deep Learning**.  \n",
    "\n",
    "**ðŸ“š RECOMMENDED RESOURCES**\n",
    "\n",
    "- *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by AurÃ©lien Geron  \n",
    "  (really great book. easy and practical)  \n",
    "\n",
    "- *Deep Learning* by Ian Goodfellow  \n",
    "  \"Deep Learning\" by Ian Goodfellow (really great book but a bit more mathematics-heavy. Great for understanding deeply the maths. Highly suggest for the part on MLPs and how they work)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the transformer blocks are preceeded by the encodng layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt](images/encoder_decoder_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an encoder-decoder sequence is not new to the Transformer architecture. What's new is using only the attention mechanism. Although RNNs were used also before, they are not great: using RNNs is not the best. Long sequences kept in memory lead to a vanishing gradient. A solution was created by the discovery of the Attention mechanism by (Bahdanau, Cho, Bengio) [Attention Paper (2014)](https://arxiv.org/abs/1409.0473). Even though the full the true power of attention , and what drives the amazing abilities of large language models, was first explored in the well-know [Attention is all you need paper (2017)](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "Attention allows a model to focus \"attend\" just to the the most relevant parts. Attention selectively determines which words are most important in a given sentence. For instance, the output word â€œlamaâ€™sâ€ is Dutch for â€œllamas,â€ which is why the attention between both is high. Similarly, the words â€œlamaâ€™sâ€ and â€œIâ€ have lower attention since they arenâ€™t as related. \n",
    "![Alt](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding these attention mechanisms to the decoder step, the RNN can\n",
    "generate signals for each input word in the sequence related to the potential\n",
    "output. Instead of passing only a context embedding to the decoder, the\n",
    "hidden states of all input words are passed. This process is demonstrated in the\n",
    "Figure \n",
    "\n",
    "![Alt](images/attention_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoding layer\n",
    "\n",
    "takes words as input, embeds them trough an embedding matrix. As we saw in the foundations, we cannot feed a machine just with words. We need numbers. Numbers will have to define not only the specific words, but also grammatical relationships. \n",
    "This is the job of the embedding matrix. It will embed both specific words and also the position of such words (positional embeddings)\n",
    "\n",
    "after the transformer blocks, we do have an unembedding matrix. \n",
    "task of the unembedding matrix is to unembed the vector. As we saw, we need numbers (vectors) to compute operations and actually be able to predict the next word. but actually, we clearly need the final word at the end. Therefore, we need to \"unembed\" the vector, and , through a softmax, choose the most probable word (the predicted word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoder-Decoder; Encoder only; Decoder-only.\n",
    "in the case of the tranformers, the encoder and decoder components are stacked on top of each other. \n",
    "this differs from other architectures, such as **BERT** that is encoder-only, or **GPT**, which is decoder-only. \n",
    "In fact, the vanilla version of BERT, which is encoder-only, can only create a contextual embedding. On the other hand, GPT, which is decoder-only, can only generate output and words.\n",
    "\n",
    "the original Transformers in an encoder-decoder architecture. This works well for some tasks, such as Machine Translation, but cannot be used for different tasks, such as text classification.\n",
    "\n",
    "For these kind of tasks, different architecture were created.\n",
    "An example is the [**bidirectional encoder representations from transformers (2018: BERT)**](https://arxiv.org/abs/1810.04805).\n",
    "Bert is an encoder-only structure. This means that his task is only creating representations of the word embeddings. It focuses only on representing language. It only uses the encoder while removes the decoder entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORMER BLOCKS ARE STACKED \n",
    "\n",
    "transformer blocks are stacked one over the other.\n",
    "\n",
    "**Why this??**\n",
    "the first blocks learn representation of words. following blocks, instead, learn more complex representations, such as the grammatical rules and semantic structures of the sentence. A column might contain 12 to 96 or more stacked blocks.\n",
    "\n",
    "\n",
    "**Words are represented as vectors of a fixed size**\n",
    "\n",
    "We have seen that words are represented as vectors. The dimension of these vectors is defined initially and stays consistent through all the transformer blocks. Usually, the choice is a vector of 512, 768, 1024 or more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INSIDE A TRANSFORMER BLOCK**\n",
    "\n",
    "a transformer block is composed by two layers\n",
    "\n",
    "- the **ATTENTION LAYER**.\n",
    "- the **FEEDFORWARD LAYER**\n",
    "\n",
    "the attention layer is mainly concerned with giving the right amount of \"attention\" or, rather, we can say, importance to the right words, in order to predict the next one.\n",
    "\n",
    "On the other hand, the feedforward layer houses the majority of the modelâ€™s processing.\n",
    "\n",
    "\n",
    "On one hand, the attention layer has the task of spotting the most relevant tokens in the input sentence. On the other hand, it is in the feedforward layer (collectively in all the model layers) that the information of which will the next probable token is stored. The information comes from all the training data on which the feedforward layer was trained. When the model was successfully trained to model a massive text archive (which included many mentions of the input), it learned and stored the information (and behaviors) that make it succeed at this task.\n",
    "\n",
    "> If you need a refresh on how a feedforward neural network works, I highly suggest you to check the two books in the \"prerequisites section\", specifically [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) or, alternatively, for a very comprehensive understanding from a mathematical view, [Deep Learning](https://www.deeplearningbook.org/)\n",
    "\n",
    "Actually, it is not just pure memorization. The model is able to interpolate between data points and more\n",
    "complex patterns to be able to generalizeâ€”which means doing well on inputs it hadnâ€™t seen in the past and were not in its training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The attention layer\n",
    "\n",
    "Context is vital in order to properly model language. Simple memorization and interpolation based on the previous token can only take us so far. We\n",
    "know that because this was one of the leading approaches to build language models before neural networks. (see [N-gram Language Models](../01_FOUNDATIONS/01_NGRAM.ipynb))\n",
    "\n",
    "Attention is a mechanism that helps the model incorporate context as itâ€™sprocessing a specific token. Think of the following prompt:\n",
    "\n",
    "â€œThe dog chased the squirrel because itâ€\n",
    "\n",
    "For the model to predict what comes after â€œit,â€ it needs to know what â€œitâ€ refers to. Does it refer to the dog or the squirrel? In a trained Transformer LLM, the attention mechanism makes that determination. Attention adds information from the context into the representation of the â€œitâ€ token. The self-attention layer incorporates relevant information from previous positions that\n",
    "help process the current token. The model does that based on the patterns seen and learned from the training dataset. Perhaps previous sentences also give more clues, like, for example, referring to the dog as â€œsheâ€ thus making it clear that â€œitâ€ refers to the squirrel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION IS ALL YOU NEED. \n",
    "\n",
    "\n",
    "## RESTART FROM HERE . (pag 133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CONTEXT**\n",
    "\n",
    "If you followed the constant updates of recent years (time of the writing: 2025), you may have noticed that consistent updates were given about the increase in context, and they were the \"best\" updates, especially in 2023. If you tried to use ChatGPT in 2022/2023 vs 2024/2025 you may have noticed that, while initially you couldn't give a very long input , now you can add also very long documents on which ask questions and get answers. Also, when you look at the characteristics of LLMs when choosing one, you may notice that it is always defined the context, and the context computes both the input and the output.\n",
    "\n",
    "This happens because, as we said, the output in an autoregressive model is computed one token at a time. To do so, the entire input is given and does one forward pass. The input is NOT given one token at a time. As we have seen, the major improvment done by the attention mechansim and the introduction of GPUs, is about being able to process text in parallel. Think about it. If we were processing data one token at a time, we wouldn't be able to make the most of the attention-mechanism, and therefore of paying \"attention\" to the most relevant parts ! Yet, our model is autoregressive, left-to-right, and we can produce one token per time, by considering all the previous n token. This is why the forward pass is done for every single token, with the increasing context given. We do say that \n",
    "\n",
    "> each token generation step is one forward pass through the model (thatâ€™s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph).\n",
    ">\n",
    "> After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt\n",
    "\n",
    "> ***transformers process sequences in parallel through attention, yet generation happens one token at a time***\n",
    "\n",
    "![Alt](images/context_autoregressive_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model generates \"Dear\" after the first forward pass, that token doesn't just get tacked onto some internal memory. Instead, the software creates a completely new input sequence: \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. Dear\" and feeds this entire extended sequence through all the transformer layers again for the second forward pass. This gives us \"Sarah.\"\n",
    "Then it happens again: \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. Dear Sarah\" goes through all the layers to predict the next token, perhaps a comma or period.\n",
    "This repetitive reprocessing is why context matters so much. Each new token benefits from the full attention mechanism applied to the entire sequence so far. The word \"Sarah\" in position 15 can attend to and be influenced by every single token that came before it, including the original prompt. This is how the model maintains coherence across long passages and why it can refer back to earlier parts of the conversation or follow complex instructions.\n",
    "Now you can see why these models are computationally intensive. If you're generating a 500-word response, you're not just doing 500 forward passes through the model. You're doing 500 forward passes where the first pass processes maybe 50 tokens, the second processes 51 tokens, the third processes 52 tokens, and so on. \n",
    "\n",
    "> ### ***The computational cost grows quadratically with sequence length.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereâ€™s a specific word used in machine learning to describe models that consume their earlier predictions to make later predictions (e.g., the modelâ€™s\n",
    "first generated token is used to generate the second token). Theyâ€™re called ***autoregressive models***. \n",
    "That is why youâ€™ll hear text generation LLMs being called **autoregressive models**. This is often used to differentiate ***text generation models*** from ***text representation models*** like BERT, which are not autoregressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from self-attention to [**FLASH ATTENTION** (2022)](https://arxiv.org/abs/2205.14135)\n",
    "\n",
    "We have seen that the context, and therefore the text fed to the model, increases with each token generated, since it is given to the model again, for each forward pass. Therefore, we should expect the generation of text to become increasingly slower (since the model has to process all the text again)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary : \n",
    "\n",
    "- RNNs process inputs token by token: at time step t, you need the hidden state from time step t-1. \n",
    "- This means: each step depends on the previous one, so you canâ€™t compute them in parallel â€” even with a GPU.\n",
    "- GPUs sit mostly idle, waiting for sequential computations to finish.\n",
    "\n",
    "Transformers: Parallelism Friendly\n",
    "- Transformers compute all token representations at once (self-attention allows looking at all tokens simultaneously).\n",
    "- No time-step dependencies like in RNNs.\n",
    "- Enables parallel matrix multiplications over entire sequences â€” which GPUs handle extremely efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers model may use a lot of energy consumption. training a model may use the energy consumption of running a household for several years ! Moreover, they may not even fit the RAM of a computer. \n",
    "A possible solution that has been developed is using **distillation**. How does distillation work ? You traing a distilled model (such as **DistillBERT**) by using as label data the predictions of the \"parent\" model - in this case, BERT. surprisingly, in this way, the resulting model performs better than if it was directly trained on the same training data the parent model was trained on. And it is way more portable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
