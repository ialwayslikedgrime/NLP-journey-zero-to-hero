{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs can be either closed or open source. When saying \"close\" we mean that they are proprietary. With this, it is usually meant that the parameters are not shared with us. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is the powerful library where we can find a lot of models, and various frameworks for working with LLMs exist.\n",
    "\n",
    "although, clearly, there are some benefits with using proprietary LLMs (such as ChatGPT or Claude). One of these is that you don't have to use your own computer power, but you can rather use these giants' computing power and server.\n",
    "\n",
    "One open source model I suggest is using the \n",
    "### **Phi-3-mini**\n",
    "which is a relatively small (3.8 billion parameters) but quite performative model. Due to its small size, the model can be run on devices with less than 8 GB of VRAM. If you performa quantization you can use even less that 6 GB of VRAM. Additionally, the model is licensed under the MIT license, which allows the model to be used for commercial purposs without constraints.\n",
    "\n",
    "(VRAM is different from RAM: while RAM is used among all the processes (sucha apps) in the computer, the VRAM is used specifically for the GPU-heavy tasks). In the case of the newest macbooks though, that are using the MX processors, the ram is \"unified\" / \"shared\" among the CPU and the GPU.\n",
    "\n",
    "\n",
    "Another advantage of the closed source models, usually, is that they have a nice interface such as ChatGPT . Sometimes all we just want is a ChatGPT-like interface with a local LLM. Fortunately, there are many incredible frameworks that allow for this. A few examples include **text-generation-webui, KoboldCpp, and LM Studio.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "running the following code \n",
    "\n",
    "\n",
    "When you use an LLM, two models are loaded:\n",
    "- The generative model itself\n",
    "- Its underlying tokenizer. (remember we've seen here: [link] that how to tokenize is not obvious). The tokenizer is in charge of splitting the input text into tokens before feeding it to the generative mode.\n",
    "\n",
    "Running the following code will start downloading the model and depending on your internet connection can take a couple of minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#if you do have a NVIDIA GPU use this code:\\n\\n\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n# Load model and tokenizer\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n\\n    device_map=\"cuda\",\\n\\n    torch_dtype=\"auto\",\\n\\n    trust_remote_code=True,\\n)\\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#if you do have a NVIDIA GPU use this code:\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "\n",
    "    device_map=\"cuda\",\n",
    "\n",
    "    torch_dtype=\"auto\",\n",
    "\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45afc5bb2c434a96a9a3cfda3c02270c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a730d4cb0a57415aa67a622bdb411faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ba4b68dc9649e4ba54abf74fe6ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dee2dc6c58049fcaecf4afae38122cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98dbed70f9e4ec1925d3d36b64c1082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267f869823c84b24a1215e546ed8ec88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f763e063e1284962b279603d8bcbfcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-08-20T07:13:25.820325Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(Connect, Custom { kind: Other, error: Custom { kind: UnexpectedEof, error: \\\"tls handshake eof\\\" } }) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-08-20T07:13:25.820391Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(Connect, Custom { kind: Other, error: Custom { kind: UnexpectedEof, error: \\\"tls handshake eof\\\" } }) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-08-20T07:13:25.820458Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.569611823s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n",
      "{\"timestamp\":\"2025-08-20T07:13:25.820476Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.052445717s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n",
      "{\"timestamp\":\"2025-08-20T07:13:28.180181Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-08-20T07:13:28.180256Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 1.214516946s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n",
      "{\"timestamp\":\"2025-08-20T07:14:07.976728Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-08-20T07:14:07.976914Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #1. Sleeping 5.487613588s before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n",
      "{\"timestamp\":\"2025-08-20T07:19:22.118394Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, source: hyper_util::client::legacy::Error(SendRequest, hyper::Error(IncompleteMessage)) }). Retrying...\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":242}\n",
      "{\"timestamp\":\"2025-08-20T07:19:22.118430Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 697.697347ms before the next attempt\"},\"filename\":\"/Users/runner/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    }
   ],
   "source": [
    "# if you do have a Macbook MX run this code\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Detect if MPS (Apple Silicon GPU) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"   # Use Apple GPU\n",
    "else:\n",
    "    device = \"cpu\"   # Fallback\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    torch_dtype=torch.float16,   # works better on MPS\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have enough to start generating text, there is a nice trick in transformers that simplifies the process, namely transformers.pipeline.\n",
    "It encapsulates the model, tokenizer, and text generation process into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are worth mentioning:\n",
    "\n",
    "- **`return_full_text`**  \n",
    "  By setting this to `False`, the prompt will not be returned but merely the output of the model.\n",
    "\n",
    "- **`max_new_tokens`**  \n",
    "  The maximum number of tokens the model will generate. By setting a limit, we prevent long and unwieldy output as some models might continue generating output until they reach their context window.\n",
    "\n",
    "- **`do_sample`**  \n",
    "  Whether the model uses a sampling strategy to choose the next token. By setting this to `False`, the model will always select the next most probable token.  \n",
    "  We will explore several sampling parameters that invoke some creativity in the model’s output.\n",
    "\n",
    "---\n",
    "\n",
    "To generate our first text, let’s instruct the model to tell a joke about chickens.  \n",
    "To do so, we format the prompt in a list of dictionaries where each dictionary relates to an entity in the conversation.  \n",
    "Our role is that of `\"user\"` and we use the `\"content\"` key to define our prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
